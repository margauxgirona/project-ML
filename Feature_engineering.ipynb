{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ff30f3",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85fbd8",
   "metadata": {},
   "source": [
    "## Library importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b248b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd1d61",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bd20a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"X_train_NHkHMNU.csv\")\n",
    "y = pd.read_csv(\"y_train_ZAN5mwg.csv\")\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df = df.drop(df.columns[-2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b55da7",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is a key step in a machine Learning project. This step prepares the data for the models. Here are the steps we followed to prepare the dataset : \n",
    "\n",
    "**Remove columns that have -1 correlation**\n",
    "\n",
    "Some vairables have -1 correlation :\n",
    "- `DE_NET_EXPORT` and `DE_NET_IMPORT`\n",
    "- `FR_NET_EXPORT` and `FR_NET_IMPORT`\n",
    "- `DE_FR_EXCHANGE` and `FR_DE_EXCHANGE`\n",
    "\n",
    "Moreover they have the same correlation with the other variables. So keeping both variables doesn't add meaning full information. That is why we chose to drop one of the variables from each -1 correlation.\n",
    "\n",
    "**Remove `FR_COAL` variable**\n",
    "\n",
    "This variable is not diversified. Thus its values are not interesting to keep.\n",
    "\n",
    "**Split the dataset**\n",
    "\n",
    "As decided thanks to the data analysis, we splited the dataset into two : french and german dataset.\n",
    "\n",
    "**Remove Nan Values from both dataset**\n",
    "\n",
    "The proportion of Nan values as well as the few rows we have for each dataset were the reasons why we chose to replace nan values by the median of each column.\n",
    "\n",
    "**Create additionnal columns according to a Threshold**\n",
    "\n",
    "Seuils pour df_fr\n",
    "- COAL_RET < 0.8\n",
    "- FR_CONSUMPTION > 1.5\n",
    "- FR_NUCLEAR < -1.8\n",
    "- FR_HYDRO < -0.4\n",
    "\n",
    "Seuils pour df_de\n",
    "- DE_CONSUMPTION > 1.2\n",
    "- DE_NET_EXPORT > -0.45\n",
    "- DE_WINDPOW > 0.3\n",
    "\n",
    "Transformation \"ReLu\"\n",
    "\n",
    "**Remove Columns that have a low correlation with the TARGET variable**\n",
    "\n",
    "Each variables whose spearman corelation with the `TARGET` variable is lower than 0.05 will be removed from the dataset. We don't consider those variables to have a correlation high enough to have a positive impact on models' performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa2b1a",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cfbcb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_fr = {\"COAL_RET\": [0.8, \"inf\"],\n",
    "                \"FR_CONSUMPTION\": [1.5, \"sup\"],\n",
    "                \"FR_NUCLEAR\": [-1.8, \"inf\"],\n",
    "                \"FR_HYDRO\":[-0.4, \"inf\"]                \n",
    "                }\n",
    "\n",
    "threshold_de = {\"DE_CONSUMPTION\": [1.2, \"sup\"],\n",
    "                \"DE_NET_EXPORT\": [-0.45, \"sup\"],\n",
    "                \"DE_WINDPOW\": [0.3, \"sup\"]\n",
    "}\n",
    "\n",
    "# COLONNES RECUPEREES TEMPORAIREMENT A LA MAIN CAR SEPARATIONN DES FICHIERS ANALYSES ET ENGINEERING\n",
    "# A RECUPER DES VARIBALES QUAND LE RASSEMBLEMENT DES FICHIERS SERA FAIT\n",
    "columns_kept_fr = [\"DE_NET_EXPORT\",\n",
    "                \"DE_HYDRO\",\n",
    "                \"DE_WINDPOW\",\n",
    "                \"FR_WINDPOW\",\n",
    "                \"GAS_RET\",\n",
    "                \"CARBON_RET\"]\n",
    "\n",
    "columns_kept_de = [\"DE_NET_EXPORT\",\n",
    "                \"DE_GAS\",\n",
    "                \"DE_COAL\",\n",
    "                \"DE_HYDRO\",\n",
    "                \"DE_WINDPOW\",\n",
    "                \"FR_WINDPOW\",\n",
    "                \"DE_LIGNITE\",\n",
    "                \"DE_RESIDUAL_LOAD\",\n",
    "                \"DE_WIND\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d32c2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns):\n",
    "    for c in columns:\n",
    "        df.drop(columns=c, inplace=True, errors=\"ignore\")\n",
    "\n",
    "def compute_median(df):\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    medians = df[numeric_cols].median()\n",
    "    return medians\n",
    "\n",
    "def missing_values_changed_with_median(df, medians):\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(medians[numeric_cols])\n",
    "    return df\n",
    "\n",
    "def add_threshold_columns(df: pd.DataFrame, column_name: str, threshold: float, way: str):\n",
    "    message = column_name + \"_THRESHOLD_\" + str(threshold)\n",
    "    # when way = \"sup\", we want to keep only values that are higher than the threshold\n",
    "    # else we keep the values that are lower than the threshold\n",
    "    if way == \"sup\":\n",
    "        df[message] = df[column_name].where(df[column_name] >= threshold, 0)\n",
    "    else:\n",
    "        df[message] = df[column_name].where(df[column_name] <= threshold, 0)\n",
    "\n",
    "def compute_quantiles(df, low = 0.25, high = 0.75, coeff=5):\n",
    "    bounds = {}\n",
    "    for column in df.select_dtypes(include=[\"number\"]).columns:\n",
    "        Q1 = df[column].quantile(low)\n",
    "        Q3 = df[column].quantile(high)\n",
    "        delta = Q3 - Q1\n",
    "        lower_bound = Q1 - coeff * delta\n",
    "        upper_bound = Q3 + coeff * delta\n",
    "        bounds[column] = (lower_bound, upper_bound)\n",
    "    return bounds\n",
    "\n",
    "def outliers_filter(df, bounds):\n",
    "    filter_ = pd.Series(True, index=df.index)\n",
    "    for column, (low, high) in bounds.items():\n",
    "        if column in df.columns:\n",
    "            filter_ &= (df[column] >= low) & (df[column] <= high)\n",
    "    return filter_\n",
    "\n",
    "def feature_engineering(df, medians, threshold, columns_kept):\n",
    "    # remove unecessary columns\n",
    "    columns_name = [\"DE_NET_IMPORT\", \"FR_NET_IMPORT\", \"DE_FR_EXCHANGE\"]\n",
    "    drop_columns(df, columns_name)\n",
    "\n",
    "    # remove FR_COAL\n",
    "    drop_columns(df, [\"FR_COAL\"])\n",
    "\n",
    "    # modify missing values\n",
    "    df = missing_values_changed_with_median(df, medians)\n",
    "\n",
    "    # add threshold columns to the french dataset\n",
    "    for key, value in threshold.items():\n",
    "        add_threshold_columns(df, key, value[0], value[1])\n",
    "\n",
    "    # drop columns that are not in the list or that have not _THRESHOLD_ in their name\n",
    "    to_keep = [c for c in df.columns if (c in columns_kept) or (\"_THRESHOLD_\" in c)]\n",
    "    df = df[to_keep]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_one_country(df, threshold, columns_kept, standardisation = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"TARGET\"]), df[\"TARGET\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    medians = compute_median(X_train)\n",
    "\n",
    "    X_train = feature_engineering(X_train, medians, threshold, columns_kept)\n",
    "    X_test = feature_engineering(X_test, medians, threshold, columns_kept)\n",
    "\n",
    "    # filter : remove outliers from the train data\n",
    "    bounds = compute_quantiles(X_train)\n",
    "    filter_ = outliers_filter(X_train, bounds)\n",
    "    X_train = X_train[filter_]\n",
    "    y_train = y_train[filter_]\n",
    "\n",
    "    if standardisation:\n",
    "        # Standardisation\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "        # X_train_fr_scaled, X_test_fr_scaled, X_train_de_scaled, X_test_de_scaled are not dataframe, \n",
    "        # we prefer to work with dataframe to keep columns name\n",
    "        X_train = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "        X_test  = pd.DataFrame(X_test_scaled,  index=X_test.index,  columns=X_test.columns)\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def transform(df, threshold_fr, threshold_de, columns_kept_fr, columns_kept_de, standardisation):\n",
    "    # split the dataset\n",
    "    df_fr = df[df[\"COUNTRY\"] == \"FR\"].copy()\n",
    "    df_de = df[df[\"COUNTRY\"] == \"DE\"].copy()\n",
    "\n",
    "    X_train_fr, X_test_fr, y_train_fr, y_test_fr = transform_one_country(\n",
    "        df_fr, threshold_fr, columns_kept_fr, standardisation=standardisation\n",
    "    )\n",
    "\n",
    "    X_train_de, X_test_de, y_train_de, y_test_de = transform_one_country(\n",
    "        df_de, threshold_de, columns_kept_de, standardisation=standardisation\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_fr, X_test_fr, y_train_fr, y_test_fr,\n",
    "        X_train_de, X_test_de, y_train_de, y_test_de\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9d0c6",
   "metadata": {},
   "source": [
    "## Pipeline for all models\n",
    "\n",
    "We observe that if our features engineering seems very relevant for simple and interpretable models, however models that handle better the complexity and non linear relationsip didn't require as feature engineering than a simple linear regression. For that purpose the goal of this part is to do a general pipeline using the last feature engineering pipeline to have a flexible way of testing new models. Furthermore since the observation of an important part of outliers in the French side, make the relationships very noisy, we will remove the extreme outliers, only on training data. We also aim to have the possibilitie to use a different model for France and Allemagne since the optimal model for each could be different. Finally in our objective to avoid overfitting we will use K-fold optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57380a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_corr(y_true, y_pred):\n",
    "    return spearmanr(y_true, y_pred).correlation\n",
    "\n",
    "def kfold_score(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model_ = clone(model)  \n",
    "        model_.fit(X_train, y_train)\n",
    "        y_pred = model_.predict(X_val)\n",
    "\n",
    "        scores.append(spearman_corr(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "def build_bagging_decision_trees(fr_tree_params, de_tree_params, fr_bagging_params, de_bagging_params):\n",
    "    fr_tree = DecisionTreeRegressor(random_state=42, **fr_tree_params)\n",
    "    de_tree = DecisionTreeRegressor(random_state=42, **de_tree_params)\n",
    "\n",
    "    bagging_fr = BaggingRegressor(estimator=fr_tree, **fr_bagging_params)\n",
    "    bagging_de = BaggingRegressor(estimator=de_tree, **de_bagging_params)\n",
    "\n",
    "    return bagging_fr, bagging_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f159aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_all(\n",
    "    df,\n",
    "    fr_model,\n",
    "    de_model,\n",
    "    threshold_fr=threshold_fr,\n",
    "    threshold_de=threshold_de,\n",
    "    columns_kept_fr=columns_kept_fr,\n",
    "    columns_kept_de=columns_kept_de,\n",
    "    feature_engineering=True,\n",
    "    standardisation=True,\n",
    "    use_grid=False,\n",
    "    k=5,\n",
    "    cv_mode_label=None,\n",
    "):\n",
    "    # comparison with or without feature engineering\n",
    "    if feature_engineering:\n",
    "        (X_train_fr, X_test_fr, y_train_fr, y_test_fr,\n",
    "            X_train_de, X_test_de, y_train_de, y_test_de) = transform(\n",
    "                                                            df,\n",
    "                                                            threshold_fr=threshold_fr,\n",
    "                                                            threshold_de=threshold_de,\n",
    "                                                            columns_kept_fr=columns_kept_fr,\n",
    "                                                            columns_kept_de=columns_kept_de,\n",
    "                                                            standardisation=standardisation)\n",
    "    else:\n",
    "        df_fr = df[df[\"COUNTRY\"] == \"FR\"].drop(columns=\"COUNTRY\").fillna(0)\n",
    "        df_de = df[df[\"COUNTRY\"] == \"DE\"].drop(columns=\"COUNTRY\").fillna(0)\n",
    "        X_train_fr, X_test_fr, y_train_fr, y_test_fr = train_test_split(df_fr.drop(columns=[\"TARGET\"]), df_fr[\"TARGET\"], test_size=0.2, random_state=42)\n",
    "        X_train_de, X_test_de, y_train_de, y_test_de = train_test_split(df_de.drop(columns=[\"TARGET\"]), df_de[\"TARGET\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    if use_grid:\n",
    "        # france\n",
    "        fr_model.fit(X_train_fr, y_train_fr)\n",
    "        fr_mean = fr_model.best_score_\n",
    "        fr_cv_scores = fr_model.cv_results_[\"mean_test_score\"]\n",
    "        fr_std = fr_cv_scores.std()\n",
    "        fr_estimator = fr_model.best_estimator_\n",
    "\n",
    "        # germany\n",
    "        de_model.fit(X_train_de, y_train_de)\n",
    "        de_mean = de_model.best_score_\n",
    "        de_cv_scores = de_model.cv_results_[\"mean_test_score\"]\n",
    "        de_std = de_cv_scores.std()\n",
    "        de_estimator = de_model.best_estimator_\n",
    "    else:\n",
    "        # k_fold\n",
    "        fr_mean, fr_std = kfold_score(fr_model, X_train_fr, y_train_fr, k=k)\n",
    "        de_mean, de_std = kfold_score(de_model, X_train_de, y_train_de, k=k)\n",
    "\n",
    "        fr_estimator = fr_model\n",
    "        de_estimator = de_model\n",
    "\n",
    "        fr_estimator.fit(X_train_fr, y_train_fr)\n",
    "        de_estimator.fit(X_train_de, y_train_de)\n",
    "\n",
    "    # Test evaluation\n",
    "    y_pred_test_fr = fr_estimator.predict(X_test_fr)\n",
    "    y_pred_test_de = de_estimator.predict(X_test_de)\n",
    "\n",
    "    fr_test_score = spearman_corr(y_test_fr, y_pred_test_fr)\n",
    "    de_test_score = spearman_corr(y_test_de, y_pred_test_de)\n",
    "\n",
    "    # Global Spearman\n",
    "    y_true_global = np.concatenate([y_test_fr, y_test_de])\n",
    "    y_pred_global = np.concatenate([y_pred_test_fr, y_pred_test_de])\n",
    "    spearman_global = spearman_corr(y_true_global, y_pred_global)\n",
    "    mse_global = mean_squared_error(y_true_global, y_pred_global)\n",
    "\n",
    "    mode_label = cv_mode_label or (\"grid_search\" if use_grid else \"kfold\")\n",
    "\n",
    "    return {\n",
    "    \"model_fr\" : fr_model,\n",
    "    \"model_de\" : de_model,\n",
    "    \"cv_mode\": mode_label,\n",
    "    \"fr_cv\": (fr_mean, fr_std),\n",
    "    \"de_cv\": (de_mean, de_std),\n",
    "    \"spearman_fr_test\": fr_test_score,\n",
    "    \"spearman_de_test\": de_test_score,\n",
    "    \"spearman_global_test\": spearman_global,\n",
    "    \"MSE\": mse_global,\n",
    "    \"features_engineering\": feature_engineering,\n",
    "    \"standardisation\":standardisation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424818f",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ee03d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_cols = [\n",
    "    \"model_fr\",\n",
    "    \"model_de\",\n",
    "    \"spearman_global_test\",\n",
    "    \"spearman_fr_test\",\n",
    "    \"spearman_de_test\",\n",
    "    \"MSE\",\n",
    "    \"cv_mode\",\n",
    "    \"features_engineering\",\n",
    "    \"standardisation\"\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(columns=allowed_cols)\n",
    "\n",
    "def display(results):\n",
    "    for key, value in results.items():\n",
    "        print(key, \":\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2102cfc",
   "metadata": {},
   "source": [
    "### Basic Model\n",
    "\n",
    "The first step is to test the simpliest model with almost no feature engineering, to have a sort of reference model and to not considerate all the models less performant. In this first implementation the dataset isn't separate between France and Germany, all the columns are keep and there is no transformation on the columns. The model used is a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddee24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman train : 28.9%\n",
      "Spearman test  : 19.5%\n"
     ]
    }
   ],
   "source": [
    "X_all = df.drop(columns=[\"TARGET\", \"COUNTRY\"]).fillna(0)\n",
    "y_all = df[\"TARGET\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test  = lr.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Spearman train : {:.1f}%\".format(100 * spearman_corr(y_train, y_pred_train)))\n",
    "print(\"Spearman test  : {:.1f}%\".format(100 * spearman_corr(y_test,  y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8828b",
   "metadata": {},
   "source": [
    "## Models with our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44daba9",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cfee0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : LinearRegression()\n",
      "model_de : LinearRegression()\n",
      "cv_mode : kfold\n",
      "fr_cv : (0.20356753106076667, 0.08232517437563475)\n",
      "de_cv : (0.24022632190504875, 0.12521555406740847)\n",
      "spearman_fr_test : 0.1657655733347872\n",
      "spearman_de_test : 0.392464221824687\n",
      "spearman_global_test : 0.2709933537010029\n",
      "MSE : 1.1774733159426616\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "res = pipeline_all(df, LinearRegression(), LinearRegression())\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c9480",
   "metadata": {},
   "source": [
    "We can see an important improvement of our spearman score, with an improvement of 8% comparing to the reference model (from 19% to 27%). This justify our global strategy at least for Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f869b",
   "metadata": {},
   "source": [
    "### Polynomiale Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7dae8831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('poly', PolynomialFeatures(include_bias=False)),\n",
      "                ('ridge', Ridge())])\n",
      "model_de : Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('poly', PolynomialFeatures(include_bias=False)),\n",
      "                ('ridge', Ridge())])\n",
      "cv_mode : kfold\n",
      "fr_cv : (0.0831742698325312, 0.09960668914965122)\n",
      "de_cv : (0.06913111813283533, 0.11194525938918111)\n",
      "spearman_fr_test : 0.23828583668485145\n",
      "spearman_de_test : 0.24842911449016106\n",
      "spearman_global_test : 0.2535651523084517\n",
      "MSE : 1.1711722141032943\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "poly_model_fr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"ridge\", Ridge(alpha=1.0))\n",
    "])\n",
    "poly_model_de = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"ridge\", Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "res = pipeline_all(df, poly_model_fr, poly_model_de)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02458ab0",
   "metadata": {},
   "source": [
    "With polynomial regression, we keep improving our performance, however this model seems adapted only for the french dataset an hybrid model (polynomial regression for the french dataset and linear regression for the deutsh one) could be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a8bf7",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929fbd1",
   "metadata": {},
   "source": [
    "### A simple hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b273dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('poly', PolynomialFeatures(include_bias=False)),\n",
      "                ('ridge', Ridge())])\n",
      "model_de : LinearRegression()\n",
      "cv_mode : kfold\n",
      "fr_cv : (0.0831742698325312, 0.09960668914965122)\n",
      "de_cv : (0.24022632190504875, 0.12521555406740847)\n",
      "spearman_fr_test : 0.23828583668485145\n",
      "spearman_de_test : 0.392464221824687\n",
      "spearman_global_test : 0.3100040267132282\n",
      "MSE : 1.154544975333234\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "res = pipeline_all(df, poly_model_fr, LinearRegression())\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42840a9",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "219f2960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('model',\n",
      "                                        DecisionTreeRegressor(random_state=42))]),\n",
      "             n_jobs=1,\n",
      "             param_grid={'model__max_depth': [3, 4, 5, 7],\n",
      "                         'model__min_samples_leaf': [10, 20, 50],\n",
      "                         'model__min_samples_split': [10, 20, 30]},\n",
      "             scoring=make_scorer(spearman_corr, response_method='predict'))\n",
      "model_de : GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('model',\n",
      "                                        DecisionTreeRegressor(random_state=42))]),\n",
      "             n_jobs=1,\n",
      "             param_grid={'model__max_depth': [3, 4, 5, 7, 10, 15],\n",
      "                         'model__min_samples_leaf': [5, 10, 20, 30, 50],\n",
      "                         'model__min_samples_split': [5, 10, 20]},\n",
      "             scoring=make_scorer(spearman_corr, response_method='predict'))\n",
      "cv_mode : grid_search\n",
      "fr_cv : (0.07108673777153425, 0.03427368833895691)\n",
      "de_cv : (0.250134185098134, 0.035309153641252665)\n",
      "spearman_fr_test : 0.13872316906362192\n",
      "spearman_de_test : 0.22403112608507902\n",
      "spearman_global_test : 0.18923562853899611\n",
      "MSE : 1.2336359335342533\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "spearman_score = make_scorer(spearman_corr, greater_is_better=True)\n",
    "fr_param_grid = {\n",
    "    \"model__max_depth\": [3, 4, 5, 7],\n",
    "    \"model__min_samples_leaf\": [10, 20, 50],\n",
    "    \"model__min_samples_split\": [10, 20, 30]\n",
    "}\n",
    "\n",
    "fr_base = Pipeline([(\"model\", DecisionTreeRegressor(random_state=42))])\n",
    "\n",
    "fr_search = GridSearchCV(\n",
    "    estimator=fr_base,\n",
    "    param_grid=fr_param_grid,\n",
    "    scoring=spearman_score,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "de_param_grid = {\n",
    "    \"model__max_depth\": [3, 4, 5, 7, 10, 15],\n",
    "    \"model__min_samples_leaf\": [5, 10, 20, 30, 50],\n",
    "    \"model__min_samples_split\": [5, 10, 20]\n",
    "}\n",
    "\n",
    "de_base = Pipeline([(\"model\", DecisionTreeRegressor(random_state=42))])\n",
    "\n",
    "de_search = GridSearchCV(\n",
    "    estimator=de_base,\n",
    "    param_grid=de_param_grid,\n",
    "    scoring=spearman_score,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "res = pipeline_all(df, fr_model=fr_search, de_model=de_search, use_grid=True)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ff1fb",
   "metadata": {},
   "source": [
    "#### Decision Tree (k-fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14e64f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : DecisionTreeRegressor(max_depth=5, min_samples_leaf=5, min_samples_split=20,\n",
      "                      random_state=42)\n",
      "model_de : DecisionTreeRegressor(max_depth=3, min_samples_leaf=20, min_samples_split=5,\n",
      "                      random_state=42)\n",
      "cv_mode : kfold\n",
      "fr_cv : (0.14971576303737627, 0.05351063373639738)\n",
      "de_cv : (0.20157967688667733, 0.1391080397964053)\n",
      "spearman_fr_test : 0.0706679694385563\n",
      "spearman_de_test : 0.22403112608507902\n",
      "spearman_global_test : 0.14901607641289508\n",
      "MSE : 1.2009603644125721\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "fr_tree_params = {\"max_depth\": 5, \"min_samples_leaf\": 5, \"min_samples_split\": 20}\n",
    "de_tree_params = {\"max_depth\": 3, \"min_samples_leaf\": 20, \"min_samples_split\": 5}\n",
    "\n",
    "fr_tree = DecisionTreeRegressor(random_state=42, **fr_tree_params)\n",
    "de_tree = DecisionTreeRegressor(random_state=42, **de_tree_params)\n",
    "\n",
    "res = pipeline_all(df, fr_tree, de_tree)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe435c",
   "metadata": {},
   "source": [
    "Decision trees performed significantly worse than linear and polynomial models. Despite extensive hyperparameter, the models failed to capture stable relationships, showing high variance and bad generalisation. This suggests that the dataset does not exhibit strong hierarchical or rule-based patterns, and tree-based splits are overly sensitive to noise, especially for the French subset, which contains many extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c630942",
   "metadata": {},
   "source": [
    "#### Decision Tree with Bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dcdec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : BaggingRegressor(bootstrap=False,\n",
      "                 estimator=DecisionTreeRegressor(max_depth=5,\n",
      "                                                 min_samples_leaf=5,\n",
      "                                                 min_samples_split=20,\n",
      "                                                 random_state=42),\n",
      "                 n_estimators=30, n_jobs=-1, random_state=42)\n",
      "model_de : BaggingRegressor(bootstrap=False,\n",
      "                 estimator=DecisionTreeRegressor(max_depth=3,\n",
      "                                                 min_samples_leaf=20,\n",
      "                                                 min_samples_split=5,\n",
      "                                                 random_state=42),\n",
      "                 max_features=0.7, max_samples=0.9, n_estimators=100, n_jobs=-1,\n",
      "                 random_state=42)\n",
      "cv_mode : bagging\n",
      "fr_cv : (0.14971576303737627, 0.05351063373639738)\n",
      "de_cv : (0.2098943397625738, 0.11103529895649739)\n",
      "spearman_fr_test : 0.0706679694385563\n",
      "spearman_de_test : 0.2106894727123582\n",
      "spearman_global_test : 0.1643289607200026\n",
      "MSE : 1.1799125692599415\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "fr_bagging_params = {\n",
    "    \"n_estimators\": 30,\n",
    "    \"max_samples\": 1.0,\n",
    "    \"max_features\": 1.0,\n",
    "    \"bootstrap\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "de_bagging_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_samples\": 0.9,\n",
    "    \"max_features\": 0.7,\n",
    "    \"bootstrap\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "bagging_fr, bagging_de = build_bagging_decision_trees(\n",
    "    fr_tree_params,\n",
    "    de_tree_params,\n",
    "    fr_bagging_params,\n",
    "    de_bagging_params,\n",
    ")\n",
    "\n",
    "res = pipeline_all(df, bagging_fr, bagging_de, cv_mode_label=\"bagging\")\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1268ee",
   "metadata": {},
   "source": [
    "Although bagging reduced variance compared to standalone trees, performance remained inferior to simpler linear models. The improvement on the German dataset was low, but the French dataset remained fine. The results indicate that ensembling does not sufficiently stabilize trees when the underlying signal-to-noise ratio is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74183a75",
   "metadata": {},
   "source": [
    "### Support Vector Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd877a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : SVR(C=10, kernel='linear')\n",
      "model_de : SVR(C=10, kernel='linear')\n",
      "cv_mode : kfold\n",
      "fr_cv : (0.21711805518813038, 0.10847272088092935)\n",
      "de_cv : (0.27390404160235177, 0.1278410551302806)\n",
      "spearman_fr_test : 0.22184475834882816\n",
      "spearman_de_test : 0.37359682468694105\n",
      "spearman_global_test : 0.30793333611858115\n",
      "MSE : 1.1950700484630297\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "svr_params = {\"C\": 10, \"kernel\": \"linear\", \"gamma\": \"scale\", \"epsilon\": 0.1}\n",
    "\n",
    "svr_fr = SVR(**svr_params)\n",
    "svr_de = SVR(**svr_params)\n",
    "\n",
    "res = pipeline_all(df, svr_fr, svr_de)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff0845b",
   "metadata": {},
   "source": [
    "SVR handles outliers more robustly than for classical linear regression, which may explain the particularly strong performance on the German dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c67ac",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e53502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : RandomForestRegressor(max_depth=8, min_samples_leaf=20, n_estimators=300,\n",
      "                      n_jobs=-1, random_state=42)\n",
      "model_de : RandomForestRegressor(max_depth=10, min_samples_leaf=10, n_estimators=400,\n",
      "                      n_jobs=-1, random_state=42)\n",
      "cv_mode : kfold\n",
      "fr_cv : (0.14865402867054464, 0.07712237886188927)\n",
      "de_cv : (0.24473379313375196, 0.1083541617965652)\n",
      "spearman_fr_test : 0.19730253996123978\n",
      "spearman_de_test : 0.2264031753130591\n",
      "spearman_global_test : 0.22525993633706437\n",
      "MSE : 1.1874674076592997\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "rf_fr = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=8,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf_de = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "res = pipeline_all(df, rf_fr, rf_de)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89ca8a",
   "metadata": {},
   "source": [
    "Random Forests did not outperform simpler regressors. Even if their ability to model complex interactions, they suffered from noise and lack of strong partition structures in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd2d04",
   "metadata": {},
   "source": [
    "### XGBoost Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c08fd13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             feature_weights=None, gamma=None, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
      "             n_jobs=-1, num_parallel_tree=None, ...)\n",
      "model_de : XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             feature_weights=None, gamma=None, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
      "             n_jobs=-1, num_parallel_tree=None, ...)\n",
      "cv_mode : kfold\n",
      "fr_cv : (0.14730962090728503, 0.05142050402107569)\n",
      "de_cv : (0.24668077275927355, 0.12999827800298283)\n",
      "spearman_fr_test : 0.18569262587460197\n",
      "spearman_de_test : 0.22833184257602868\n",
      "spearman_global_test : 0.22061143817664072\n",
      "MSE : 1.207922387633466\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"reg_lambda\": 2.0,\n",
    "    \"reg_alpha\": 1.0,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "xgb_fr = XGBRegressor(**xgb_params)\n",
    "xgb_de = XGBRegressor(**xgb_params)\n",
    "\n",
    "res = pipeline_all(df, xgb_fr, xgb_de)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb23c0",
   "metadata": {},
   "source": [
    "XGBoost performed slightly better than Random Forests. Even after parameter tuning, the gains still limited. This may indicates that boosting is unable to compensate for the limited structure and relatively low signal-to-noise ratio of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b4a2c",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "we already tested linear, polynomial, tree-based and boosting models on the engineered features.\n",
    "Does a neural network capture non-linear patterns that classical ML models cannot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46aa647",
   "metadata": {},
   "source": [
    "#### What's Deep Learning?\n",
    "\n",
    "It's important to distinguish:\n",
    "- The classic Machine learning models: Not very deep (linear regression, SVR, random forest, XGBoost…).\n",
    "- Deep Learning: neural networks with multiple layers, capable of approaching highly non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f05668",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "\n",
    "Lago, J., De Ridder, F., & De Schutter, B. (2018).  \n",
    "*Forecasting spot electricity prices: Deep learning approaches and empirical comparison of traditional algorithms.*  \n",
    "Applied Energy, 221, 386–405.  \n",
    "[https://www.sciencedirect.com/science/article/pii/S030626191830196X](https://www.sciencedirect.com/science/article/pii/S030626191830196X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c91db",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84342189",
   "metadata": {},
   "source": [
    "### Feed-forward neural networks for tabular regression\n",
    "\n",
    "In our principal paper reference, our problem is called a **tabular regression**, indeed, each row of our dataset is an independent sample with no explicit temporal structure, so we are not in the case of time-series modelling.\n",
    "\n",
    "For this type of data, the most natural deep learning model is a **feed-forward neural network**, also called a **Multi-Layer Perceptron (MLP)**. An MLP implements a non-linear regression of the form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f_\\theta(x)\n",
    "$$\n",
    "\n",
    "where $x$ is the input feature vector and $\\hat{y}$ is the predicted day-ahead price (TARGET).  \n",
    "The network is composed of:\n",
    "\n",
    "- **An input layer** whose dimension equals the number of engineered features for France or Germany.\n",
    "- **Several hidden layers** of fully connected (dense) neurons with non-linear activation functions, here typically $\\text{ReLU}(z) = \\max(0, z)$. These layers allow the network to model complex, non-linear interactions between features.\n",
    "- **A single output neuron** with **linear activation**, providing a real-valued price prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### What the neural network truly minimizes\n",
    "\n",
    "During training, the neural network does **not** optimise the Spearman correlation directly.  \n",
    "Instead, it learns its parameters $\\theta$ by minimising a **differentiable loss function** (the Spearman correlation is not differentiable), usually the **Mean Absolute Error (MAE)** or **Mean Squared Error (MSE)**:\n",
    "\n",
    "**MAE:**\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "**MSE:**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "More sophisticated architectures are also proposed by the article of J. Lago for electricity markets: **recurrent neural networks (RNN, LSTM/GRU)** to exploit hourly temporal dependencies, and **convolutional or hybrid CNN–LSTM models**.  \n",
    "However, these models require access to high-frequency time series and lead to more complex experimental setups (sequence modelling, larger datasets, heavier hyperparameter tuning).\n",
    "\n",
    "In this work, we deliberately focus on a **simple MLP for tabular regression**, trained on the **same engineered features** as our classical machine learning models.  \n",
    "This choice keeps the comparison fair and transparent: any performance difference can be attributed to the modelling capacity of the neural network, rather than to a change in the input representation or in the evaluation protocol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a74ee",
   "metadata": {},
   "source": [
    "### Methodology: how we use Deep Learning in this project\n",
    "\n",
    "#### Data and features used\n",
    "\n",
    "For the deep learning experiments, we keep **exactly the same data preparation pipeline** as for the classical models.  \n",
    "This includes:\n",
    "\n",
    "- splitting the dataset into **FR** and **DE** subsets,  \n",
    "- filling missing values with the **median**,  \n",
    "- creating threshold-based variables and interaction features,  \n",
    "- removing features with very low correlation to the target,  \n",
    "- applying **standardisation** using StandardScaler.\n",
    "\n",
    "Keeping the same preprocessing ensures a **fair and consistent comparison** between neural networks and the traditional machine learning approaches used earlier.\n",
    "\n",
    "---\n",
    "\n",
    "#### First neural network architecture\n",
    "\n",
    "We train **one neural network per country** (FR and DE), which is consistent with the structure of our `pipeline_all` function.\n",
    "\n",
    "A first simple baseline architecture is used:\n",
    "\n",
    "- **FR model:**  \n",
    "  Dense(64, ReLU) → Dense(32, ReLU) → Dense(1, linear)\n",
    "\n",
    "\n",
    "- **DE model:**  \n",
    "  Aproximately the same architecture.\n",
    "\n",
    "The networks use:\n",
    "\n",
    "- **He initialization** for the hidden layers, all the weights of the hidden layers are the same,\n",
    "- **MSE** as the loss function,  \n",
    "- **Adam** as the optimizer, it's what update the weights.\n",
    "\n",
    "This architecture is intentionally lightweight to keep the comparison with classical models fair and interpretable.\n",
    "\n",
    "\n",
    "### Regularisation and overfitting control\n",
    "\n",
    "Because our dataset is relatively small, a neural network can easily **overfit**.  \n",
    "To reduce this risk, we incorporate several regularisation mechanisms:\n",
    "\n",
    "- **L2 weight decay** (`kernel_regularizer`):  \n",
    "  penalises large weights during training, encouraging the network to learn smoother and simpler representations.\n",
    "\n",
    "- **Dropout** between hidden layers (e.g. 0.2–0.3):  \n",
    "  randomly deactivates a fraction of neurons at each training step, preventing the network from relying too heavily on specific connections.\n",
    "\n",
    "- **Early stopping** based on validation loss:  \n",
    "  stops training automatically when the model stops improving, avoiding unnecessary optimisation that would lead to overfitting.\n",
    "\n",
    "These techniques help stabilise the training process and improve the model’s ability to generalise to unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8296358",
   "metadata": {},
   "source": [
    "### Deep learning implementation: libraries and integration with the pipeline\n",
    "\n",
    "To implements the Deep learning models we use the library TensorFlows and Keras. However the Neurol Network build from Keras can't be use directly in our pipeline, especially because contrary to a model like LinearRegression() from scikit-learn, a keras model don't have some important features (.fit, .predict etc). The following class deals with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64ba44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class KerasWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 hidden_units=(64, 32),\n",
    "                 dropout_rate=0.0,\n",
    "                 epochs=80,\n",
    "                 batch_size=32,\n",
    "                 verbose=0):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self, input_dim):\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # Hidden layers\n",
    "        for units in self.hidden_units:\n",
    "            model.add(layers.Dense(units, activation=\"relu\",\n",
    "                                   kernel_initializer=\"he_normal\"))\n",
    "            if self.dropout_rate > 0:\n",
    "                model.add(layers.Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X).astype(\"float32\")\n",
    "        y = np.asarray(y).astype(\"float32\")\n",
    "\n",
    "        self.model = self.build_model(X.shape[1])\n",
    "\n",
    "        self.model.fit(\n",
    "            X, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=self.verbose,\n",
    "            validation_split=0.1,\n",
    "            callbacks=[keras.callbacks.EarlyStopping(\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )]\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X).astype(\"float32\")\n",
    "        preds = self.model.predict(X, verbose=0)\n",
    "        return preds.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d290a1",
   "metadata": {},
   "source": [
    "### Experimental Evaluation of the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a101cf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : KerasWrapper(epochs=50)\n",
      "model_de : KerasWrapper(epochs=50)\n",
      "cv_mode : kfold\n",
      "fr_cv : (0.013745038526499475, 0.085970309331849)\n",
      "de_cv : (0.1637630865706696, 0.028055232781786708)\n",
      "spearman_fr_test : 0.12396748264065077\n",
      "spearman_de_test : 0.2793269230769231\n",
      "spearman_global_test : 0.2065614980439978\n",
      "MSE : 1.296559201953897\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "fr_model = KerasWrapper(epochs=50, batch_size=32)\n",
    "de_model = KerasWrapper(epochs=50, batch_size=32)\n",
    "\n",
    "results = pipeline_all(\n",
    "    df=df,\n",
    "    fr_model=fr_model,\n",
    "    de_model=de_model,\n",
    "    feature_engineering=True,\n",
    "    standardisation=True,\n",
    "    use_grid=False,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "display(results)\n",
    "df_results.loc[len(df_results)] = {key: results[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5601a",
   "metadata": {},
   "source": [
    "We obtain a pretty low MSE comparing to the other models which is logical since it's the optimized metric by our model, however which is more surprising is that the obtain Spearman correlation is correct even if it's not the optimized metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc881378",
   "metadata": {},
   "source": [
    "## Optimization of hyperparameter for our Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ff409",
   "metadata": {},
   "source": [
    "##### Hyperparameters to optimize:\n",
    "- **Hidden layer sizes**: The number of neurons in each hidden layer.\n",
    "- **Dropout rate**: Dropout helps prevent overfitting by randomly deactivating a fraction of neurons during training.\n",
    "\n",
    "- **Batch size**: Defines how many samples are processed before each weight update. It influences training stability and convergence speed, we notably wan't to increase Batch size because we find different results at each execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: {'hidden_units': (32, 16), 'dropout': 0.0}\n",
      "Testing: {'hidden_units': (64, 32), 'dropout': 0.0}\n",
      "Testing: {'hidden_units': (64, 32), 'dropout': 0.2}\n",
      "Testing: {'hidden_units': (128, 64, 32), 'dropout': 0.1}\n",
      "Testing: {'hidden_units': (64, 64), 'dropout': 0.1}\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\"hidden_units\": (32, 16), \"dropout\": 0.0},\n",
    "    {\"hidden_units\": (64, 32), \"dropout\": 0.0},\n",
    "    {\"hidden_units\": (64, 32), \"dropout\": 0.2},\n",
    "    {\"hidden_units\": (128, 64, 32), \"dropout\": 0.1},\n",
    "    {\"hidden_units\": (64, 64), \"dropout\": 0.1},\n",
    "]\n",
    "\n",
    "results_dl = []\n",
    "\n",
    "for params in param_grid:\n",
    "    print(\"Testing:\", params)\n",
    "\n",
    "    fr_model = KerasWrapper(\n",
    "        hidden_units=params[\"hidden_units\"],\n",
    "        dropout_rate=params[\"dropout\"],\n",
    "        epochs=80,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    de_model = KerasWrapper(\n",
    "        hidden_units=params[\"hidden_units\"],\n",
    "        dropout_rate=params[\"dropout\"],\n",
    "        epochs=80,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    res = pipeline_all(\n",
    "        df=df,\n",
    "        fr_model=fr_model,\n",
    "        de_model=de_model,\n",
    "        feature_engineering=True,\n",
    "        standardisation=True,\n",
    "        use_grid=False,\n",
    "        k=3\n",
    "    )\n",
    "\n",
    "    results_dl.append((params, res[\"spearman_global_test\"]))\n",
    "\n",
    "    results_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5827955f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'hidden_units': (32, 16), 'dropout': 0.0}, 0.040052018378882996),\n",
       " ({'hidden_units': (64, 32), 'dropout': 0.0}, 0.17858784145951895),\n",
       " ({'hidden_units': (64, 32), 'dropout': 0.2}, 0.22076255103394488),\n",
       " ({'hidden_units': (128, 64, 32), 'dropout': 0.1}, 0.2472148567050352),\n",
       " ({'hidden_units': (64, 64), 'dropout': 0.1}, 0.21706961947117978)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56d056",
   "metadata": {},
   "source": [
    "### Our best MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61960ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : KerasWrapper(dropout_rate=0.1, hidden_units=(128, 64, 32))\n",
      "model_de : KerasWrapper(dropout_rate=0.1, hidden_units=(128, 64, 32))\n",
      "cv_mode : kfold\n",
      "fr_cv : (0.032795982689726784, 0.0841978223180897)\n",
      "de_cv : (0.2012457495983868, 0.027021123513288302)\n",
      "spearman_fr_test : 0.1585092270317485\n",
      "spearman_de_test : 0.23986471377459756\n",
      "spearman_global_test : 0.21284023726498497\n",
      "MSE : 1.2287081512094298\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "fr_model = KerasWrapper(\n",
    "        hidden_units=(128, 64, 32),\n",
    "        dropout_rate=0.1,\n",
    "        epochs=80,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "de_model = KerasWrapper(\n",
    "    hidden_units=(128, 64, 32),\n",
    "    dropout_rate=0.1,\n",
    "    epochs=80,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    "    )\n",
    "\n",
    "res = pipeline_all(\n",
    "    df=df,\n",
    "    fr_model=fr_model,\n",
    "    de_model=de_model,\n",
    "    feature_engineering=True,\n",
    "    standardisation=True,\n",
    "    use_grid=False,\n",
    "    k=3\n",
    "    )\n",
    "\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f08f2a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_fr</th>\n",
       "      <th>model_de</th>\n",
       "      <th>spearman_global_test</th>\n",
       "      <th>spearman_fr_test</th>\n",
       "      <th>spearman_de_test</th>\n",
       "      <th>MSE</th>\n",
       "      <th>cv_mode</th>\n",
       "      <th>features_engineering</th>\n",
       "      <th>standardisation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(StandardScaler(), PolynomialFeatures(include_...</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>0.310004</td>\n",
       "      <td>0.238286</td>\n",
       "      <td>0.392464</td>\n",
       "      <td>1.154545</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVR(C=10, kernel='linear')</td>\n",
       "      <td>SVR(C=10, kernel='linear')</td>\n",
       "      <td>0.307933</td>\n",
       "      <td>0.221845</td>\n",
       "      <td>0.373597</td>\n",
       "      <td>1.195070</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>0.270993</td>\n",
       "      <td>0.165766</td>\n",
       "      <td>0.392464</td>\n",
       "      <td>1.177473</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(StandardScaler(), PolynomialFeatures(include_...</td>\n",
       "      <td>(StandardScaler(), PolynomialFeatures(include_...</td>\n",
       "      <td>0.253565</td>\n",
       "      <td>0.238286</td>\n",
       "      <td>0.248429</td>\n",
       "      <td>1.171172</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=8, max_featur...</td>\n",
       "      <td>(DecisionTreeRegressor(max_depth=10, max_featu...</td>\n",
       "      <td>0.225260</td>\n",
       "      <td>0.197303</td>\n",
       "      <td>0.226403</td>\n",
       "      <td>1.187467</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>0.220611</td>\n",
       "      <td>0.185693</td>\n",
       "      <td>0.228332</td>\n",
       "      <td>1.207922</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KerasWrapper(epochs=50)</td>\n",
       "      <td>KerasWrapper(epochs=50)</td>\n",
       "      <td>0.206561</td>\n",
       "      <td>0.123967</td>\n",
       "      <td>0.279327</td>\n",
       "      <td>1.296559</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=Pip...</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=Pip...</td>\n",
       "      <td>0.189236</td>\n",
       "      <td>0.138723</td>\n",
       "      <td>0.224031</td>\n",
       "      <td>1.233636</td>\n",
       "      <td>grid_search</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=5, min_sample...</td>\n",
       "      <td>(DecisionTreeRegressor(max_depth=3, min_sample...</td>\n",
       "      <td>0.164329</td>\n",
       "      <td>0.070668</td>\n",
       "      <td>0.210689</td>\n",
       "      <td>1.179913</td>\n",
       "      <td>bagging</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, min_samples...</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=3, min_samples...</td>\n",
       "      <td>0.149016</td>\n",
       "      <td>0.070668</td>\n",
       "      <td>0.224031</td>\n",
       "      <td>1.200960</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_fr  \\\n",
       "2  (StandardScaler(), PolynomialFeatures(include_...   \n",
       "6                         SVR(C=10, kernel='linear')   \n",
       "0                                 LinearRegression()   \n",
       "1  (StandardScaler(), PolynomialFeatures(include_...   \n",
       "7  (DecisionTreeRegressor(max_depth=8, max_featur...   \n",
       "8  XGBRegressor(base_score=None, booster=None, ca...   \n",
       "9                            KerasWrapper(epochs=50)   \n",
       "3  GridSearchCV(cv=5,\\n             estimator=Pip...   \n",
       "5  (DecisionTreeRegressor(max_depth=5, min_sample...   \n",
       "4  DecisionTreeRegressor(max_depth=5, min_samples...   \n",
       "\n",
       "                                            model_de  spearman_global_test  \\\n",
       "2                                 LinearRegression()              0.310004   \n",
       "6                         SVR(C=10, kernel='linear')              0.307933   \n",
       "0                                 LinearRegression()              0.270993   \n",
       "1  (StandardScaler(), PolynomialFeatures(include_...              0.253565   \n",
       "7  (DecisionTreeRegressor(max_depth=10, max_featu...              0.225260   \n",
       "8  XGBRegressor(base_score=None, booster=None, ca...              0.220611   \n",
       "9                            KerasWrapper(epochs=50)              0.206561   \n",
       "3  GridSearchCV(cv=5,\\n             estimator=Pip...              0.189236   \n",
       "5  (DecisionTreeRegressor(max_depth=3, min_sample...              0.164329   \n",
       "4  DecisionTreeRegressor(max_depth=3, min_samples...              0.149016   \n",
       "\n",
       "   spearman_fr_test  spearman_de_test       MSE      cv_mode  \\\n",
       "2          0.238286          0.392464  1.154545        kfold   \n",
       "6          0.221845          0.373597  1.195070        kfold   \n",
       "0          0.165766          0.392464  1.177473        kfold   \n",
       "1          0.238286          0.248429  1.171172        kfold   \n",
       "7          0.197303          0.226403  1.187467        kfold   \n",
       "8          0.185693          0.228332  1.207922        kfold   \n",
       "9          0.123967          0.279327  1.296559        kfold   \n",
       "3          0.138723          0.224031  1.233636  grid_search   \n",
       "5          0.070668          0.210689  1.179913      bagging   \n",
       "4          0.070668          0.224031  1.200960        kfold   \n",
       "\n",
       "   features_engineering  standardisation  \n",
       "2                  True             True  \n",
       "6                  True             True  \n",
       "0                  True             True  \n",
       "1                  True             True  \n",
       "7                  True             True  \n",
       "8                  True             True  \n",
       "9                  True             True  \n",
       "3                  True             True  \n",
       "5                  True             True  \n",
       "4                  True             True  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=\"spearman_global_test\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603e3bc",
   "metadata": {},
   "source": [
    "We observe poor performance, especially for the German dataset. Essayer d'optimiser ça???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
