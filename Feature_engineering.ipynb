{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ff30f3",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85fbd8",
   "metadata": {},
   "source": [
    "## Library importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b248b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd1d61",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7bd20a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"X_train_NHkHMNU.csv\")\n",
    "y = pd.read_csv(\"y_train_ZAN5mwg.csv\")\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df = df.drop(df.columns[-2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b55da7",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is a key step in a machine Learning project. This step prepares the data for the models. Here are the steps we followed to prepare the dataset : \n",
    "\n",
    "**Remove columns that have -1 correlation**\n",
    "\n",
    "Some vairables have -1 correlation :\n",
    "- `DE_NET_EXPORT` and `DE_NET_IMPORT`\n",
    "- `FR_NET_EXPORT` and `FR_NET_IMPORT`\n",
    "- `DE_FR_EXCHANGE` and `FR_DE_EXCHANGE`\n",
    "\n",
    "Moreover they have the same correlation with the other variables. So keeping both variables doesn't add meaning full information. That is why we chose to drop one of the variables from each -1 correlation.\n",
    "\n",
    "**Remove `FR_COAL` variable**\n",
    "\n",
    "This variable is not diversified. Thus its values are not interesting to keep.\n",
    "\n",
    "**Split the dataset**\n",
    "\n",
    "As decided thanks to the data analysis, we splited the dataset into two : french and german dataset.\n",
    "\n",
    "**Remove Nan Values from both dataset**\n",
    "\n",
    "The proportion of Nan values as well as the few rows we have for each dataset were the reasons why we chose to replace nan values by the median of each column.\n",
    "\n",
    "**Create additionnal columns according to a Threshold**\n",
    "\n",
    "Seuils pour df_fr\n",
    "- COAL_RET < 0.8\n",
    "- FR_CONSUMPTION > 1.5\n",
    "- FR_NUCLEAR < -1.8\n",
    "- FR_HYDRO < -0.4\n",
    "\n",
    "Seuils pour df_de\n",
    "- DE_CONSUMPTION > 1.2\n",
    "- DE_NET_EXPORT > -0.45\n",
    "- DE_WINDPOW > 0.3\n",
    "\n",
    "Transformation \"ReLu\"\n",
    "\n",
    "**Remove Columns that have a low correlation with the TARGET variable**\n",
    "\n",
    "Each variables whose spearman corelation with the `TARGET` variable is lower than 0.05 will be removed from the dataset. We don't consider those variables to have a correlation high enough to have a positive impact on models' performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa2b1a",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cfbcb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_fr = {\"COAL_RET\": [0.8, \"inf\"],\n",
    "                \"FR_CONSUMPTION\": [1.5, \"sup\"],\n",
    "                \"FR_NUCLEAR\": [-1.8, \"inf\"],\n",
    "                \"FR_HYDRO\":[-0.4, \"inf\"]                \n",
    "                }\n",
    "\n",
    "threshold_de = {\"DE_CONSUMPTION\": [1.2, \"sup\"],\n",
    "                \"DE_NET_EXPORT\": [-0.45, \"sup\"],\n",
    "                \"DE_WINDPOW\": [0.3, \"sup\"]\n",
    "}\n",
    "\n",
    "# COLONNES RECUPEREES TEMPORAIREMENT A LA MAIN CAR SEPARATIONN DES FICHIERS ANALYSES ET ENGINEERING\n",
    "# A RECUPER DES VARIBALES QUAND LE RASSEMBLEMENT DES FICHIERS SERA FAIT\n",
    "columns_kept_fr = [\"DE_NET_EXPORT\",\n",
    "                \"DE_HYDRO\",\n",
    "                \"DE_WINDPOW\",\n",
    "                \"FR_WINDPOW\",\n",
    "                \"GAS_RET\",\n",
    "                \"CARBON_RET\"]\n",
    "\n",
    "columns_kept_de = [\"DE_NET_EXPORT\",\n",
    "                \"DE_GAS\",\n",
    "                \"DE_COAL\",\n",
    "                \"DE_HYDRO\",\n",
    "                \"DE_WINDPOW\",\n",
    "                \"FR_WINDPOW\",\n",
    "                \"DE_LIGNITE\",\n",
    "                \"DE_RESIDUAL_LOAD\",\n",
    "                \"DE_WIND\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d32c2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns):\n",
    "    for c in columns:\n",
    "        df.drop(columns=c, inplace=True, errors=\"ignore\")\n",
    "\n",
    "def compute_median(df):\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    medians = df[numeric_cols].median()\n",
    "    return medians\n",
    "\n",
    "def missing_values_changed_with_median(df, medians):\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(medians[numeric_cols])\n",
    "    return df\n",
    "\n",
    "def add_threshold_columns(df: pd.DataFrame, column_name: str, threshold: float, way: str):\n",
    "    message = column_name + \"_THRESHOLD_\" + str(threshold)\n",
    "    # when way = \"sup\", we want to keep only values that are higher than the threshold\n",
    "    # else we keep the values that are lower than the threshold\n",
    "    if way == \"sup\":\n",
    "        df[message] = df[column_name].where(df[column_name] >= threshold, 0)\n",
    "    else:\n",
    "        df[message] = df[column_name].where(df[column_name] <= threshold, 0)\n",
    "\n",
    "def compute_quantiles(df, low = 0.25, high = 0.75, coeff=5):\n",
    "    bounds = {}\n",
    "    for column in df.select_dtypes(include=[\"number\"]).columns:\n",
    "        Q1 = df[column].quantile(low)\n",
    "        Q3 = df[column].quantile(high)\n",
    "        delta = Q3 - Q1\n",
    "        lower_bound = Q1 - coeff * delta\n",
    "        upper_bound = Q3 + coeff * delta\n",
    "        bounds[column] = (lower_bound, upper_bound)\n",
    "    return bounds\n",
    "\n",
    "def outliers_filter(df, bounds):\n",
    "    filter_ = pd.Series(True, index=df.index)\n",
    "    for column, (low, high) in bounds.items():\n",
    "        if column in df.columns:\n",
    "            filter_ &= (df[column] >= low) & (df[column] <= high)\n",
    "    return filter_\n",
    "\n",
    "def feature_engineering(df, medians, threshold, columns_kept):\n",
    "    # remove unecessary columns\n",
    "    columns_name = [\"DE_NET_IMPORT\", \"FR_NET_IMPORT\", \"DE_FR_EXCHANGE\"]\n",
    "    drop_columns(df, columns_name)\n",
    "\n",
    "    # remove FR_COAL\n",
    "    drop_columns(df, [\"FR_COAL\"])\n",
    "\n",
    "    # modify missing values\n",
    "    df = missing_values_changed_with_median(df, medians)\n",
    "\n",
    "    # add threshold columns to the french dataset\n",
    "    for key, value in threshold.items():\n",
    "        add_threshold_columns(df, key, value[0], value[1])\n",
    "\n",
    "    # drop columns that are not in the list or that have not _THRESHOLD_ in their name\n",
    "    to_keep = [c for c in df.columns if (c in columns_kept) or (\"_THRESHOLD_\" in c)]\n",
    "    df = df[to_keep]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_one_country(df, threshold, columns_kept, standardisation = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"TARGET\"]), df[\"TARGET\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    medians = compute_median(X_train)\n",
    "\n",
    "    X_train = feature_engineering(X_train, medians, threshold, columns_kept)\n",
    "    X_test = feature_engineering(X_test, medians, threshold, columns_kept)\n",
    "\n",
    "    # filter : remove outliers from the train data\n",
    "    bounds = compute_quantiles(X_train)\n",
    "    filter_ = outliers_filter(X_train, bounds)\n",
    "    X_train = X_train[filter_]\n",
    "    y_train = y_train[filter_]\n",
    "\n",
    "    if standardisation:\n",
    "        # Standardisation\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "        # X_train_fr_scaled, X_test_fr_scaled, X_train_de_scaled, X_test_de_scaled are not dataframe, \n",
    "        # we prefer to work with dataframe to keep columns name\n",
    "        X_train = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "        X_test  = pd.DataFrame(X_test_scaled,  index=X_test.index,  columns=X_test.columns)\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def transform(df, threshold_fr, threshold_de, columns_kept_fr, columns_kept_de, standardisation):\n",
    "    # split the dataset\n",
    "    df_fr = df[df[\"COUNTRY\"] == \"FR\"].copy()\n",
    "    df_de = df[df[\"COUNTRY\"] == \"DE\"].copy()\n",
    "\n",
    "    X_train_fr, X_test_fr, y_train_fr, y_test_fr = transform_one_country(\n",
    "        df_fr, threshold_fr, columns_kept_fr, standardisation=standardisation\n",
    "    )\n",
    "\n",
    "    X_train_de, X_test_de, y_train_de, y_test_de = transform_one_country(\n",
    "        df_de, threshold_de, columns_kept_de, standardisation=standardisation\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_fr, X_test_fr, y_train_fr, y_test_fr,\n",
    "        X_train_de, X_test_de, y_train_de, y_test_de\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9d0c6",
   "metadata": {},
   "source": [
    "## Pipeline for all models\n",
    "\n",
    "We observe that if our features engineering seems very relevant for simple and interpretable models, however models that handle better the complexity and non linear relationsip didn't require as feature engineering than a simple linear regression. For that purpose the goal of this part is to do a general pipeline using the last feature engineering pipeline to have a flexible way of testing new models. Furthermore since the observation of an important part of outliers in the French side, make the relationships very noisy, we will remove the extreme outliers, only on training data. We also aim to have the possibilitie to use a different model for France and Allemagne since the optimal model for each could be different. Finally in our objective to avoid overfitting we will use K-fold optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57380a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_corr(y_true, y_pred):\n",
    "    return spearmanr(y_true, y_pred).correlation\n",
    "\n",
    "def kfold_score(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model_ = clone(model)  \n",
    "        model_.fit(X_train, y_train)\n",
    "        y_pred = model_.predict(X_val)\n",
    "\n",
    "        scores.append(spearman_corr(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "def build_bagging_decision_trees(fr_tree_params, de_tree_params, fr_bagging_params, de_bagging_params):\n",
    "    fr_tree = DecisionTreeRegressor(random_state=42, **fr_tree_params)\n",
    "    de_tree = DecisionTreeRegressor(random_state=42, **de_tree_params)\n",
    "\n",
    "    bagging_fr = BaggingRegressor(estimator=fr_tree, **fr_bagging_params)\n",
    "    bagging_de = BaggingRegressor(estimator=de_tree, **de_bagging_params)\n",
    "\n",
    "    return bagging_fr, bagging_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f159aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_all(\n",
    "    df,\n",
    "    fr_model,\n",
    "    de_model,\n",
    "    threshold_fr=threshold_fr,\n",
    "    threshold_de=threshold_de,\n",
    "    columns_kept_fr=columns_kept_fr,\n",
    "    columns_kept_de=columns_kept_de,\n",
    "    feature_engineering=True,\n",
    "    standardisation=True,\n",
    "    use_grid=False,\n",
    "    k=5,\n",
    "    cv_mode_label=None,\n",
    "):\n",
    "    # comparison with or without feature engineering\n",
    "    if feature_engineering:\n",
    "        (X_train_fr, X_test_fr, y_train_fr, y_test_fr,\n",
    "            X_train_de, X_test_de, y_train_de, y_test_de) = transform(\n",
    "                                                            df,\n",
    "                                                            threshold_fr=threshold_fr,\n",
    "                                                            threshold_de=threshold_de,\n",
    "                                                            columns_kept_fr=columns_kept_fr,\n",
    "                                                            columns_kept_de=columns_kept_de,\n",
    "                                                            standardisation=standardisation)\n",
    "    else:\n",
    "        df_fr = df[df[\"COUNTRY\"] == \"FR\"].drop(columns=\"COUNTRY\").fillna(0)\n",
    "        df_de = df[df[\"COUNTRY\"] == \"DE\"].drop(columns=\"COUNTRY\").fillna(0)\n",
    "        X_train_fr, X_test_fr, y_train_fr, y_test_fr = train_test_split(df_fr.drop(columns=[\"TARGET\"]), df_fr[\"TARGET\"], test_size=0.2, random_state=42)\n",
    "        X_train_de, X_test_de, y_train_de, y_test_de = train_test_split(df_de.drop(columns=[\"TARGET\"]), df_de[\"TARGET\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    if use_grid:\n",
    "        # france\n",
    "        fr_model.fit(X_train_fr, y_train_fr)\n",
    "        fr_mean = fr_model.best_score_\n",
    "        fr_cv_scores = fr_model.cv_results_[\"mean_test_score\"]\n",
    "        fr_std = fr_cv_scores.std()\n",
    "        fr_estimator = fr_model.best_estimator_\n",
    "\n",
    "        # germany\n",
    "        de_model.fit(X_train_de, y_train_de)\n",
    "        de_mean = de_model.best_score_\n",
    "        de_cv_scores = de_model.cv_results_[\"mean_test_score\"]\n",
    "        de_std = de_cv_scores.std()\n",
    "        de_estimator = de_model.best_estimator_\n",
    "    else:\n",
    "        # k_fold\n",
    "        fr_mean, fr_std = kfold_score(fr_model, X_train_fr, y_train_fr, k=k)\n",
    "        de_mean, de_std = kfold_score(de_model, X_train_de, y_train_de, k=k)\n",
    "\n",
    "        fr_estimator = fr_model\n",
    "        de_estimator = de_model\n",
    "\n",
    "        fr_estimator.fit(X_train_fr, y_train_fr)\n",
    "        de_estimator.fit(X_train_de, y_train_de)\n",
    "\n",
    "    # Test evaluation\n",
    "    y_pred_test_fr = fr_estimator.predict(X_test_fr)\n",
    "    y_pred_test_de = de_estimator.predict(X_test_de)\n",
    "\n",
    "    fr_test_score = spearman_corr(y_test_fr, y_pred_test_fr)\n",
    "    de_test_score = spearman_corr(y_test_de, y_pred_test_de)\n",
    "\n",
    "    # Global Spearman\n",
    "    y_true_global = np.concatenate([y_test_fr, y_test_de])\n",
    "    y_pred_global = np.concatenate([y_pred_test_fr, y_pred_test_de])\n",
    "    spearman_global = spearman_corr(y_true_global, y_pred_global)\n",
    "\n",
    "    mode_label = cv_mode_label or (\"grid_search\" if use_grid else \"kfold\")\n",
    "\n",
    "    return {\n",
    "    \"model_fr\" : fr_model,\n",
    "    \"model_de\" : de_model,\n",
    "    \"cv_mode\": mode_label,\n",
    "    \"fr_cv\": (fr_mean, fr_std),\n",
    "    \"de_cv\": (de_mean, de_std),\n",
    "    \"spearman_fr_test\": fr_test_score,\n",
    "    \"spearman_de_test\": de_test_score,\n",
    "    \"spearman_global_test\": spearman_global,\n",
    "    \"features_engineering\": feature_engineering,\n",
    "    \"standardisation\":standardisation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424818f",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ee03d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_cols = [\n",
    "    \"model_fr\",\n",
    "    \"model_de\",\n",
    "    \"spearman_global_test\",\n",
    "    \"spearman_fr_test\",\n",
    "    \"spearman_de_test\",\n",
    "    \"cv_mode\",\n",
    "    \"features_engineering\",\n",
    "    \"standardisation\"\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(columns=allowed_cols)\n",
    "\n",
    "def display(results):\n",
    "    for key, value in results.items():\n",
    "        print(key, \":\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2102cfc",
   "metadata": {},
   "source": [
    "### Basic Model\n",
    "\n",
    "The first step is to test the simpliest model with almost no feature engineering, to have a sort of reference model and to not considerate all the models less performant. In this first implementation the dataset isn't separate between France and Germany, all the columns are keep and there is no transformation on the columns. The model used is a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddee24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman train : 28.9%\n",
      "Spearman test  : 19.5%\n"
     ]
    }
   ],
   "source": [
    "X_all = df.drop(columns=[\"TARGET\", \"COUNTRY\"]).fillna(0)\n",
    "y_all = df[\"TARGET\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test  = lr.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Spearman train : {:.1f}%\".format(100 * spearman_corr(y_train, y_pred_train)))\n",
    "print(\"Spearman test  : {:.1f}%\".format(100 * spearman_corr(y_test,  y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8828b",
   "metadata": {},
   "source": [
    "## Models with our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44daba9",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cfee0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : LinearRegression()\n",
      "model_de : LinearRegression()\n",
      "cv_mode : kfold\n",
      "fr_cv : (np.float64(0.20356753106076667), np.float64(0.08232517437563475))\n",
      "de_cv : (np.float64(0.24022632190504875), np.float64(0.12521555406740847))\n",
      "spearman_fr_test : 0.1657655733347872\n",
      "spearman_de_test : 0.392464221824687\n",
      "spearman_global_test : 0.2709933537010029\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "res = pipeline_all(df, LinearRegression(), LinearRegression())\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c9480",
   "metadata": {},
   "source": [
    "We can see an important improvement of our spearman score, with an improvement of 8% comparing to the reference model (from 19% to 27%). This justify our global strategy at least for Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f869b",
   "metadata": {},
   "source": [
    "### Polynomiale Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dae8831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : Pipeline(steps=[('poly', PolynomialFeatures(include_bias=False)),\n",
      "                ('lr', LinearRegression())])\n",
      "model_de : Pipeline(steps=[('poly', PolynomialFeatures(include_bias=False)),\n",
      "                ('lr', LinearRegression())])\n",
      "cv_mode : kfold\n",
      "fr_cv : (np.float64(0.08015539997634188), np.float64(0.09902778164029584))\n",
      "de_cv : (np.float64(0.07672971424295011), np.float64(0.10003044840653669))\n",
      "spearman_fr_test : 0.23626385164754035\n",
      "spearman_de_test : 0.25490831842576034\n",
      "spearman_global_test : 0.2508406763811741\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "poly_model_fr = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "poly_model_de = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "res = pipeline_all(df, poly_model_fr, poly_model_de)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02458ab0",
   "metadata": {},
   "source": [
    "With polynomial regression, we keep improving our performance, however this model seems adapted only for the french dataset an hybrid model (polynomial regression for the french dataset and linear regression for the deutsh one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a8bf7",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929fbd1",
   "metadata": {},
   "source": [
    "### A simple hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b273dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : Pipeline(steps=[('poly', PolynomialFeatures(include_bias=False)),\n",
      "                ('lr', LinearRegression())])\n",
      "model_de : LinearRegression()\n",
      "cv_mode : kfold\n",
      "fr_cv : (np.float64(0.08015539997634188), np.float64(0.09902778164029584))\n",
      "de_cv : (np.float64(0.24022632190504875), np.float64(0.12521555406740847))\n",
      "spearman_fr_test : 0.23626385164754035\n",
      "spearman_de_test : 0.392464221824687\n",
      "spearman_global_test : 0.30924401793090445\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "res = pipeline_all(df, poly_model_fr, LinearRegression())\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42840a9",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "219f2960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('model',\n",
      "                                        DecisionTreeRegressor(random_state=42))]),\n",
      "             n_jobs=1,\n",
      "             param_grid={'model__max_depth': [3, 4, 5, 7],\n",
      "                         'model__min_samples_leaf': [10, 20, 50],\n",
      "                         'model__min_samples_split': [10, 20, 30]},\n",
      "             scoring=make_scorer(spearman_corr, response_method='predict'))\n",
      "model_de : GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('model',\n",
      "                                        DecisionTreeRegressor(random_state=42))]),\n",
      "             n_jobs=1,\n",
      "             param_grid={'model__max_depth': [3, 4, 5, 7, 10, 15],\n",
      "                         'model__min_samples_leaf': [5, 10, 20, 30, 50],\n",
      "                         'model__min_samples_split': [5, 10, 20]},\n",
      "             scoring=make_scorer(spearman_corr, response_method='predict'))\n",
      "cv_mode : grid_search\n",
      "fr_cv : (np.float64(0.07108673777153425), np.float64(0.03427368833895691))\n",
      "de_cv : (np.float64(0.250134185098134), np.float64(0.035309153641252665))\n",
      "spearman_fr_test : 0.13872316906362192\n",
      "spearman_de_test : 0.22403112608507902\n",
      "spearman_global_test : 0.18923562853899611\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "spearman_score = make_scorer(spearman_corr, greater_is_better=True)\n",
    "fr_param_grid = {\n",
    "    \"model__max_depth\": [3, 4, 5, 7],\n",
    "    \"model__min_samples_leaf\": [10, 20, 50],\n",
    "    \"model__min_samples_split\": [10, 20, 30]\n",
    "}\n",
    "\n",
    "fr_base = Pipeline([(\"model\", DecisionTreeRegressor(random_state=42))])\n",
    "\n",
    "fr_search = GridSearchCV(\n",
    "    estimator=fr_base,\n",
    "    param_grid=fr_param_grid,\n",
    "    scoring=spearman_score,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "de_param_grid = {\n",
    "    \"model__max_depth\": [3, 4, 5, 7, 10, 15],\n",
    "    \"model__min_samples_leaf\": [5, 10, 20, 30, 50],\n",
    "    \"model__min_samples_split\": [5, 10, 20]\n",
    "}\n",
    "\n",
    "de_base = Pipeline([(\"model\", DecisionTreeRegressor(random_state=42))])\n",
    "\n",
    "de_search = GridSearchCV(\n",
    "    estimator=de_base,\n",
    "    param_grid=de_param_grid,\n",
    "    scoring=spearman_score,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "res = pipeline_all(df, fr_model=fr_search, de_model=de_search, use_grid=True)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ff1fb",
   "metadata": {},
   "source": [
    "#### Decision Tree (k-fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14e64f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : DecisionTreeRegressor(max_depth=5, min_samples_leaf=5, min_samples_split=20,\n",
      "                      random_state=42)\n",
      "model_de : DecisionTreeRegressor(max_depth=3, min_samples_leaf=20, min_samples_split=5,\n",
      "                      random_state=42)\n",
      "cv_mode : kfold\n",
      "fr_cv : (np.float64(0.14971576303737627), np.float64(0.05351063373639738))\n",
      "de_cv : (np.float64(0.20157967688667733), np.float64(0.1391080397964053))\n",
      "spearman_fr_test : 0.0706679694385563\n",
      "spearman_de_test : 0.22403112608507902\n",
      "spearman_global_test : 0.14901607641289508\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "fr_tree_params = {\"max_depth\": 5, \"min_samples_leaf\": 5, \"min_samples_split\": 20}\n",
    "de_tree_params = {\"max_depth\": 3, \"min_samples_leaf\": 20, \"min_samples_split\": 5}\n",
    "\n",
    "fr_tree = DecisionTreeRegressor(random_state=42, **fr_tree_params)\n",
    "de_tree = DecisionTreeRegressor(random_state=42, **de_tree_params)\n",
    "\n",
    "res = pipeline_all(df, fr_tree, de_tree)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe435c",
   "metadata": {},
   "source": [
    "Decision trees performed significantly worse than linear and polynomial models. Despite extensive hyperparameter, the models failed to capture stable relationships, showing high variance and bad generalisation. This suggests that the dataset does not exhibit strong hierarchical or rule-based patterns, and tree-based splits are overly sensitive to noise, especially for the French subset, which contains many extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c630942",
   "metadata": {},
   "source": [
    "#### Decision Tree with Bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3dcdec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : BaggingRegressor(bootstrap=False,\n",
      "                 estimator=DecisionTreeRegressor(max_depth=5,\n",
      "                                                 min_samples_leaf=5,\n",
      "                                                 min_samples_split=20,\n",
      "                                                 random_state=42),\n",
      "                 n_estimators=30, n_jobs=-1, random_state=42)\n",
      "model_de : BaggingRegressor(bootstrap=False,\n",
      "                 estimator=DecisionTreeRegressor(max_depth=3,\n",
      "                                                 min_samples_leaf=20,\n",
      "                                                 min_samples_split=5,\n",
      "                                                 random_state=42),\n",
      "                 max_features=0.7, max_samples=0.9, n_estimators=100, n_jobs=-1,\n",
      "                 random_state=42)\n",
      "cv_mode : bagging\n",
      "fr_cv : (np.float64(0.14971576303737627), np.float64(0.05351063373639738))\n",
      "de_cv : (np.float64(0.2098943397625738), np.float64(0.11103529895649739))\n",
      "spearman_fr_test : 0.0706679694385563\n",
      "spearman_de_test : 0.2106894727123582\n",
      "spearman_global_test : 0.1643289607200026\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "fr_bagging_params = {\n",
    "    \"n_estimators\": 30,\n",
    "    \"max_samples\": 1.0,\n",
    "    \"max_features\": 1.0,\n",
    "    \"bootstrap\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "de_bagging_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_samples\": 0.9,\n",
    "    \"max_features\": 0.7,\n",
    "    \"bootstrap\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "bagging_fr, bagging_de = build_bagging_decision_trees(\n",
    "    fr_tree_params,\n",
    "    de_tree_params,\n",
    "    fr_bagging_params,\n",
    "    de_bagging_params,\n",
    ")\n",
    "\n",
    "res = pipeline_all(df, bagging_fr, bagging_de, cv_mode_label=\"bagging\")\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1268ee",
   "metadata": {},
   "source": [
    "Although bagging reduced variance compared to standalone trees, performance remained inferior to simpler linear models. The improvement on the German dataset was low, but the French dataset remained fine. The results indicate that ensembling does not sufficiently stabilize trees when the underlying signal-to-noise ratio is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74183a75",
   "metadata": {},
   "source": [
    "### Support Vector Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd877a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : SVR(C=10, kernel='linear')\n",
      "model_de : SVR(C=10, kernel='linear')\n",
      "cv_mode : kfold\n",
      "fr_cv : (np.float64(0.21711805518813038), np.float64(0.10847272088092935))\n",
      "de_cv : (np.float64(0.27390404160235177), np.float64(0.1278410551302806))\n",
      "spearman_fr_test : 0.22184475834882816\n",
      "spearman_de_test : 0.37359682468694105\n",
      "spearman_global_test : 0.30793333611858115\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "svr_params = {\"C\": 10, \"kernel\": \"linear\", \"gamma\": \"scale\", \"epsilon\": 0.1}\n",
    "\n",
    "svr_fr = SVR(**svr_params)\n",
    "svr_de = SVR(**svr_params)\n",
    "\n",
    "res = pipeline_all(df, svr_fr, svr_de)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff0845b",
   "metadata": {},
   "source": [
    "SVR handles outliers more robustly than for classical linear regression, which may explain the particularly strong performance on the German dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c67ac",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e53502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : RandomForestRegressor(max_depth=8, min_samples_leaf=20, n_estimators=300,\n",
      "                      n_jobs=-1, random_state=42)\n",
      "model_de : RandomForestRegressor(max_depth=10, min_samples_leaf=10, n_estimators=400,\n",
      "                      n_jobs=-1, random_state=42)\n",
      "cv_mode : kfold\n",
      "fr_cv : (np.float64(0.14865402867054464), np.float64(0.07712237886188927))\n",
      "de_cv : (np.float64(0.24473379313375196), np.float64(0.1083541617965652))\n",
      "spearman_fr_test : 0.19730253996123978\n",
      "spearman_de_test : 0.2264031753130591\n",
      "spearman_global_test : 0.22525993633706437\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "rf_fr = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=8,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf_de = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "res = pipeline_all(df, rf_fr, rf_de)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89ca8a",
   "metadata": {},
   "source": [
    "Random Forests did not outperform simpler regressors. Even if their ability to model complex interactions, they suffered from noise and lack of strong partition structures in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd2d04",
   "metadata": {},
   "source": [
    "### XGBoost Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c08fd13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fr : XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             feature_weights=None, gamma=None, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
      "             n_jobs=-1, num_parallel_tree=None, ...)\n",
      "model_de : XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             feature_weights=None, gamma=None, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
      "             n_jobs=-1, num_parallel_tree=None, ...)\n",
      "cv_mode : kfold\n",
      "fr_cv : (np.float64(0.12671876636773632), np.float64(0.03497917600333644))\n",
      "de_cv : (np.float64(0.23729051335282772), np.float64(0.11896660178511997))\n",
      "spearman_fr_test : 0.21093923904966902\n",
      "spearman_de_test : 0.2218358676207514\n",
      "spearman_global_test : 0.22575683096784682\n",
      "features_engineering : True\n",
      "standardisation : True\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"reg_lambda\": 2.0,\n",
    "    \"reg_alpha\": 1.0,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "xgb_fr = XGBRegressor(**xgb_params)\n",
    "xgb_de = XGBRegressor(**xgb_params)\n",
    "\n",
    "res = pipeline_all(df, xgb_fr, xgb_de)\n",
    "display(res)\n",
    "df_results.loc[len(df_results)] = {key: res[key] for key in allowed_cols}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb23c0",
   "metadata": {},
   "source": [
    "XGBoost performed slightly better than Random Forests. Even after parameter tuning, the gains still limited. This may indicates that boosting is unable to compensate for the limited structure and relatively low signal-to-noise ratio of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c91db",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f08f2a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_fr</th>\n",
       "      <th>model_de</th>\n",
       "      <th>spearman_global_test</th>\n",
       "      <th>spearman_fr_test</th>\n",
       "      <th>spearman_de_test</th>\n",
       "      <th>cv_mode</th>\n",
       "      <th>features_engineering</th>\n",
       "      <th>standardisation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(PolynomialFeatures(include_bias=False), Linea...</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>0.309244</td>\n",
       "      <td>0.236264</td>\n",
       "      <td>0.392464</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVR(C=10, kernel='linear')</td>\n",
       "      <td>SVR(C=10, kernel='linear')</td>\n",
       "      <td>0.307933</td>\n",
       "      <td>0.221845</td>\n",
       "      <td>0.373597</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>0.270993</td>\n",
       "      <td>0.165766</td>\n",
       "      <td>0.392464</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(PolynomialFeatures(include_bias=False), Linea...</td>\n",
       "      <td>(PolynomialFeatures(include_bias=False), Linea...</td>\n",
       "      <td>0.250841</td>\n",
       "      <td>0.236264</td>\n",
       "      <td>0.254908</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>0.225757</td>\n",
       "      <td>0.210939</td>\n",
       "      <td>0.221836</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=8, max_featur...</td>\n",
       "      <td>(DecisionTreeRegressor(max_depth=10, max_featu...</td>\n",
       "      <td>0.225260</td>\n",
       "      <td>0.197303</td>\n",
       "      <td>0.226403</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=Pip...</td>\n",
       "      <td>GridSearchCV(cv=5,\\n             estimator=Pip...</td>\n",
       "      <td>0.189236</td>\n",
       "      <td>0.138723</td>\n",
       "      <td>0.224031</td>\n",
       "      <td>grid_search</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=5, min_sample...</td>\n",
       "      <td>(DecisionTreeRegressor(max_depth=3, min_sample...</td>\n",
       "      <td>0.164329</td>\n",
       "      <td>0.070668</td>\n",
       "      <td>0.210689</td>\n",
       "      <td>bagging</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=5, min_samples...</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=3, min_samples...</td>\n",
       "      <td>0.149016</td>\n",
       "      <td>0.070668</td>\n",
       "      <td>0.224031</td>\n",
       "      <td>kfold</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_fr  \\\n",
       "2  (PolynomialFeatures(include_bias=False), Linea...   \n",
       "6                         SVR(C=10, kernel='linear')   \n",
       "0                                 LinearRegression()   \n",
       "1  (PolynomialFeatures(include_bias=False), Linea...   \n",
       "8  XGBRegressor(base_score=None, booster=None, ca...   \n",
       "7  (DecisionTreeRegressor(max_depth=8, max_featur...   \n",
       "3  GridSearchCV(cv=5,\\n             estimator=Pip...   \n",
       "5  (DecisionTreeRegressor(max_depth=5, min_sample...   \n",
       "4  DecisionTreeRegressor(max_depth=5, min_samples...   \n",
       "\n",
       "                                            model_de  spearman_global_test  \\\n",
       "2                                 LinearRegression()              0.309244   \n",
       "6                         SVR(C=10, kernel='linear')              0.307933   \n",
       "0                                 LinearRegression()              0.270993   \n",
       "1  (PolynomialFeatures(include_bias=False), Linea...              0.250841   \n",
       "8  XGBRegressor(base_score=None, booster=None, ca...              0.225757   \n",
       "7  (DecisionTreeRegressor(max_depth=10, max_featu...              0.225260   \n",
       "3  GridSearchCV(cv=5,\\n             estimator=Pip...              0.189236   \n",
       "5  (DecisionTreeRegressor(max_depth=3, min_sample...              0.164329   \n",
       "4  DecisionTreeRegressor(max_depth=3, min_samples...              0.149016   \n",
       "\n",
       "   spearman_fr_test  spearman_de_test      cv_mode  features_engineering  \\\n",
       "2          0.236264          0.392464        kfold                  True   \n",
       "6          0.221845          0.373597        kfold                  True   \n",
       "0          0.165766          0.392464        kfold                  True   \n",
       "1          0.236264          0.254908        kfold                  True   \n",
       "8          0.210939          0.221836        kfold                  True   \n",
       "7          0.197303          0.226403        kfold                  True   \n",
       "3          0.138723          0.224031  grid_search                  True   \n",
       "5          0.070668          0.210689      bagging                  True   \n",
       "4          0.070668          0.224031        kfold                  True   \n",
       "\n",
       "   standardisation  \n",
       "2             True  \n",
       "6             True  \n",
       "0             True  \n",
       "1             True  \n",
       "8             True  \n",
       "7             True  \n",
       "3             True  \n",
       "5             True  \n",
       "4             True  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=\"spearman_global_test\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603e3bc",
   "metadata": {},
   "source": [
    "We observe poor performance, especially for the German dataset. Essayer d'optimiser ça???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
