{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ff30f3",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85fbd8",
   "metadata": {},
   "source": [
    "## Library importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b248b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd1d61",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd20a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"X_train_NHkHMNU.csv\")\n",
    "y = pd.read_csv(\"y_train_ZAN5mwg.csv\")\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df = df.drop(df.columns[-2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b55da7",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is a key step in a machine Learning project. This step prepares the data for the models. Here are the steps we followed to prepare the dataset : \n",
    "\n",
    "**Remove columns that have -1 correlation**\n",
    "\n",
    "Some vairables have -1 correlation :\n",
    "- `DE_NET_EXPORT` and `DE_NET_IMPORT`\n",
    "- `FR_NET_EXPORT` and `FR_NET_IMPORT`\n",
    "- `DE_FR_EXCHANGE` and `FR_DE_EXCHANGE`\n",
    "\n",
    "Moreover they have the same correlation with the other variables. So keeping both variables doesn't add meaning full information. That is why we chose to drop one of the variables from each -1 correlation.\n",
    "\n",
    "**Remove `FR_COAL` variable**\n",
    "\n",
    "This variable is not diversified. Thus its values are not interesting to keep.\n",
    "\n",
    "**Split the dataset**\n",
    "\n",
    "As decided thanks to the data analysis, we splited the dataset into two : french and german dataset.\n",
    "\n",
    "**Remove Nan Values from both dataset**\n",
    "\n",
    "The proportion of Nan values as well as the few rows we have for each dataset were the reasons why we chose to replace nan values by the median of each column.\n",
    "\n",
    "**Create additionnal columns according to a Threshold**\n",
    "\n",
    "Seuils pour df_fr\n",
    "- COAL_RET < 0.8\n",
    "- FR_CONSUMPTION > 1.5\n",
    "- FR_NUCLEAR < -1.8\n",
    "- FR_HYDRO < -0.4\n",
    "\n",
    "Seuils pour df_de\n",
    "- DE_CONSUMPTION > 1.2\n",
    "- DE_NET_EXPORT > -0.45\n",
    "- DE_WINDPOW > 0.3\n",
    "\n",
    "Transformation \"ReLu\"\n",
    "\n",
    "**Remove Columns that have a low correlation with the TARGET variable**\n",
    "\n",
    "Each variables whose spearman corelation with the `TARGET` variable is lower than 0.05 will be removed from the dataset. We don't consider those variables to have a correlation high enough to have a positive impact on models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32c2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ColumnDrop(df, columns):\n",
    "    for c in columns:\n",
    "        df.drop(columns=c, inplace=True, errors=\"ignore\")\n",
    "\n",
    "def MissingValuesChangedMedian(df):\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "def AddSeuilColumn(df: pd.DataFrame, column_name: str, seuil: float, way: str):\n",
    "    message = column_name + \"_THRESHOLD_\" + str(seuil)\n",
    "    if way == \"sup\":\n",
    "        df[message] = df[column_name].where(df[column_name] >= seuil, 0)\n",
    "    else:\n",
    "        df[message] = df[column_name].where(df[column_name] <= seuil, 0)\n",
    "\n",
    "def Pipe(df):\n",
    "    # remove unecessary columns\n",
    "    columns_name = [\"DE_NET_IMPORT\", \"FR_NET_IMPORT\", \"DE_FR_EXCHANGE\"]\n",
    "    ColumnDrop(df, columns_name)\n",
    "\n",
    "    # remove FR_COAL\n",
    "    ColumnDrop(df, [\"FR_COAL\"])\n",
    "\n",
    "    # split the dataset\n",
    "    df_fr = df[df[\"COUNTRY\"] == \"FR\"].copy()\n",
    "    df_de = df[df[\"COUNTRY\"] == \"DE\"].copy()\n",
    "\n",
    "\n",
    "\n",
    "    # modify missing values\n",
    "    MissingValuesChangedMedian(df_fr)\n",
    "    MissingValuesChangedMedian(df_de)\n",
    "\n",
    "    threshold_fr = {\"COAL_RET\": [0.8, \"inf\"],\n",
    "                \"FR_CONSUMPTION\": [1.5, \"sup\"],\n",
    "                \"FR_NUCLEAR\": [-1.8, \"inf\"],\n",
    "                \"FR_HYDRO\":[-0.4, \"inf\"]                \n",
    "                }\n",
    "\n",
    "    threshold_de = {\"DE_CONSUMPTION\": [1.2, \"sup\"],\n",
    "                    \"DE_NET_EXPORT\": [-0.45, \"sup\"],\n",
    "                    \"DE_WINDPOW\": [0.3, \"sup\"]\n",
    "    }\n",
    "\n",
    "    # add threshold columns to the french dataset\n",
    "    for key, value in threshold_fr.items():\n",
    "        AddSeuilColumn(df_fr, key, value[0], value[1])\n",
    "\n",
    "    # add threshold columns to the german dataset\n",
    "    for key, value in threshold_de.items():\n",
    "        AddSeuilColumn(df_de, key, value[0], value[1])\n",
    "\n",
    "    # COLONNES RECUPEREES TEMPORAIREMENT A LA MAIN CAR SEPARATIONN DES FICHIERS ANALYSES ET ENGINEERING\n",
    "    # A RECUPER DES VARIBALES QUAND LE RASSEMBLEMENT DES FICHIERS SERA FAIT\n",
    "    columns_keep_fr = [\"DE_NET_EXPORT\",\n",
    "                    \"DE_HYDRO\",\n",
    "                    \"DE_WINDPOW\",\n",
    "                    \"FR_WINDPOW\",\n",
    "                    \"GAS_RET\",\n",
    "                    \"CARBON_RET\",\n",
    "                    \"TARGET\"]\n",
    "\n",
    "    columns_keep_de = [\"DE_NET_EXPORT\",\n",
    "                    \"DE_GAS\",\n",
    "                    \"DE_COAL\",\n",
    "                    \"DE_HYDRO\",\n",
    "                    \"DE_WINDPOW\",\n",
    "                    \"FR_WINDPOW\",\n",
    "                    \"DE_LIGNITE\",\n",
    "                    \"DE_RESIDUAL_LOAD\",\n",
    "                    \"DE_WIND\",\n",
    "                    \"TARGET\"]\n",
    "\n",
    "    # drop columns that are not in thoses lists\n",
    "    # french\n",
    "    for c in df_fr.columns:\n",
    "        if c not in columns_keep_fr and \"_THRESHOLD_\" not in c:\n",
    "            df_fr.drop(columns=c, inplace=True)\n",
    "\n",
    "    #german\n",
    "    for c in df_de.columns:\n",
    "        if c not in columns_keep_de and \"_THRESHOLD_\" not in c:\n",
    "            df_de.drop(columns=c, inplace=True)\n",
    "    \n",
    "    return df_fr, df_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de4d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr, df_de = Pipe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9d0c6",
   "metadata": {},
   "source": [
    "## Pipeline for all models\n",
    "\n",
    "We observe that if our features engineering seems very relevant for simple and interpretable models, however models that handle better the complexity and non linear relationsip didn't require as feature engineering than a simple linear regression. For that purpose the goal of this part is to do a general pipeline using the last feature engineering pipeline to have a flexible way of testing new models. Furthermore since the observation of an important part of outliers in the French side, make the relationships very noisy, we will remove the extreme outliers, only on training data. We also aim to have the possibilitie to use a different model for France and Allemagne since the optimal model for each could be different. Finally in our objective to avoid overfitting we will use K-fold optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57380a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def spearman_corr(y_true, y_pred):\n",
    "    return spearmanr(y_true, y_pred).correlation\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "def kfold_score(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model_ = clone(model)  \n",
    "        model_.fit(X_train, y_train)\n",
    "        y_pred = model_.predict(X_val)\n",
    "\n",
    "        scores.append(spearman_corr(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "def supp_outliers(df,coeff=5):\n",
    "    for column in df.select_dtypes(include=['number']).columns:\n",
    "        Q1 = df[column].quantile(1/4)\n",
    "        Q3 = df[column].quantile(3/4)\n",
    "        delta = Q3 - Q1\n",
    "        lower_bound = Q1 - coeff * delta\n",
    "        upper_bound = Q3 + coeff * delta\n",
    "        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ce5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pipeline_All(df, fr_model, de_model, features_engineering= True, remove_outliers = True, k = 5):\n",
    "    if features_engineering == True:\n",
    "        df_fr, df_de = Pipe(df)\n",
    "    else:\n",
    "        df_fr = df[df[\"COUNTRY\"] == \"FR\"].drop(columns=\"COUNTRY\").fillna(0)\n",
    "        df_de = df[df[\"COUNTRY\"] == \"DE\"].drop(columns=\"COUNTRY\").fillna(0)\n",
    "\n",
    "    X_fr = df_fr.drop(columns=[\"TARGET\"])\n",
    "    y_fr = df_fr[\"TARGET\"]\n",
    "\n",
    "    X_de = df_de.drop(columns=[\"TARGET\"])\n",
    "    y_de = df_de[\"TARGET\"]\n",
    "\n",
    "    X_train_fr, X_test_fr, y_train_fr, y_test_fr = train_test_split(X_fr, y_fr, test_size=0.2, random_state=42)\n",
    "    X_train_de, X_test_de, y_train_de, y_test_de = train_test_split(X_de, y_de, test_size=0.2, random_state=42)\n",
    "\n",
    "    if remove_outliers:\n",
    "        temp = X_train_fr.copy()\n",
    "        temp[\"TARGET\"] = y_train_fr\n",
    "        temp = supp_outliers(temp, coeff=5)\n",
    "        X_train_fr = temp.drop(columns=[\"TARGET\"])\n",
    "        y_train_fr = temp[\"TARGET\"]\n",
    "\n",
    "\n",
    "    fr_mean, fr_std = kfold_score(fr_model, X_train_fr, y_train_fr, k=k)\n",
    "\n",
    "    de_mean, de_std = kfold_score(de_model, X_train_de, y_train_de, k=k)\n",
    "\n",
    "    fr_model.fit(X_train_fr, y_train_fr)\n",
    "    de_model.fit(X_train_de, y_train_de)\n",
    "\n",
    "    y_pred_test_fr = fr_model.predict(X_test_fr)\n",
    "    y_pred_test_de = de_model.predict(X_test_de)\n",
    "\n",
    "    fr_test_score = spearman_corr(y_test_fr, y_pred_test_fr)\n",
    "    de_test_score = spearman_corr(y_test_de, y_pred_test_de)\n",
    "\n",
    "    y_true_global = np.concatenate([y_test_fr, y_test_de])\n",
    "    y_pred_global = np.concatenate([y_pred_test_fr, y_pred_test_de])\n",
    "\n",
    "    spearman_global = spearman_corr(y_true_global, y_pred_global)\n",
    "\n",
    "    return {\n",
    "        \"fr_kfold\": (fr_mean, fr_std),\n",
    "        \"de_kfold\": (de_mean, de_std),\n",
    "        \"spearman_fr_test\": fr_test_score,\n",
    "        \"spearman_de_test\": de_test_score,\n",
    "        \"spearman_global_test\": spearman_global,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2102cfc",
   "metadata": {},
   "source": [
    "### Basic Model\n",
    "\n",
    "The first step is to test the simpliest model with almost no feature engineering, to have a sort of reference model and to not considerate all the models less performant. In this first implementation the dataset isn't separate between France and Germany, all the columns are keep and there is no transformation on the columns. The model used is a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddee24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman train : 28.9%\n",
      "Spearman test  : 19.4%\n"
     ]
    }
   ],
   "source": [
    "X_all = df.drop(columns=[\"TARGET\", \"COUNTRY\"]).fillna(0)\n",
    "y_all = df[\"TARGET\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test  = lr.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Spearman train : {:.1f}%\".format(100 * spearman_corr(y_train, y_pred_train)))\n",
    "print(\"Spearman test  : {:.1f}%\".format(100 * spearman_corr(y_test,  y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8828b",
   "metadata": {},
   "source": [
    "## Models with our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44daba9",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cfee0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fr_kfold': (0.1912701991703602, 0.1017400899045871),\n",
       " 'de_kfold': (0.3019722817513048, 0.06163762273514753),\n",
       " 'spearman_fr_test': 0.2168767951117552,\n",
       " 'spearman_de_test': 0.38370974955277287,\n",
       " 'spearman_global_test': 0.2716369166932864}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline_All(df, LinearRegression(), LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c9480",
   "metadata": {},
   "source": [
    "We can see an important improvement of our spearman score, with an amelioration of 8% comparing to the reference model (from 19% to 27%). This justify our global strategy at least for Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f869b",
   "metadata": {},
   "source": [
    "### Polynomiale Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dae8831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fr_kfold': (0.13778979428894014, 0.10676547796809663),\n",
       " 'de_kfold': (0.1761269917353287, 0.1291168563390837),\n",
       " 'spearman_fr_test': 0.2646904412907821,\n",
       " 'spearman_de_test': 0.32879025044722726,\n",
       " 'spearman_global_test': 0.29068113675983126}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_model_fr = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "poly_model_de = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "Pipeline_All(df, poly_model_fr, poly_model_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02458ab0",
   "metadata": {},
   "source": [
    "With polynomial regression, we keep improving our performance, however this model seems adapted only for the french dataset an hybrid model (polynomial regression for the french dataset and linear regression for the deutsh one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a8bf7",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929fbd1",
   "metadata": {},
   "source": [
    "### A simple hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b273dbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fr_kfold': (0.13778979428894014, 0.10676547796809663),\n",
       " 'de_kfold': (0.3019722817513048, 0.06163762273514753),\n",
       " 'spearman_fr_test': 0.2646904412907821,\n",
       " 'spearman_de_test': 0.38370974955277287,\n",
       " 'spearman_global_test': 0.30588975694833276}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline_All(df, poly_model_fr, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42840a9",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "219f2960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fr_kfold': (0.1030746921499699, 0.08823846652670929),\n",
       " 'de_kfold': (0.22776058012535377, 0.054213820605364386),\n",
       " 'spearman_fr_test': 0.24802168128052768,\n",
       " 'spearman_de_test': 0.20495986869711383,\n",
       " 'spearman_global_test': 0.1890557652292927}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearman_score = make_scorer(spearman_corr, greater_is_better=True)\n",
    "fr_param_grid = {\n",
    "    \"model__max_depth\": [3, 4, 5, 7],\n",
    "    \"model__min_samples_leaf\": [10, 20, 50],\n",
    "    \"model__min_samples_split\": [10, 20, 30]\n",
    "}\n",
    "\n",
    "fr_base = Pipeline([(\"model\", DecisionTreeRegressor(random_state=42))])\n",
    "\n",
    "fr_search = GridSearchCV(\n",
    "    estimator=fr_base,\n",
    "    param_grid=fr_param_grid,\n",
    "    scoring=spearman_score,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "de_param_grid = {\n",
    "    \"model__max_depth\": [3, 4, 5, 7, 10, 15],\n",
    "    \"model__min_samples_leaf\": [5, 10, 20, 30, 50],\n",
    "    \"model__min_samples_split\": [5, 10, 20]\n",
    "}\n",
    "\n",
    "de_base = Pipeline([(\"model\", DecisionTreeRegressor(random_state=42))])\n",
    "\n",
    "de_search = GridSearchCV(\n",
    "    estimator=de_base,\n",
    "    param_grid=de_param_grid,\n",
    "    scoring=spearman_score,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "Pipeline_All(df, fr_search, de_search, features_engineering=  True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603e3bc",
   "metadata": {},
   "source": [
    "We observe poor performance, especially for the German dataset. Essayer d'optimiser ça???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ba171",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
