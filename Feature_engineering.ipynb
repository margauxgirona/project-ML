{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ff30f3",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85fbd8",
   "metadata": {},
   "source": [
    "## Library importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b248b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd1d61",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd20a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"X_train_NHkHMNU.csv\")\n",
    "y = pd.read_csv(\"y_train_ZAN5mwg.csv\")\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df = df.drop(df.columns[-2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b55da7",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is a key step in a machine Learning project. This step prepares the data for the models. Here are the steps we followed to prepare the dataset : \n",
    "\n",
    "**Remove columns that have -1 correlation**\n",
    "\n",
    "Some vairables have -1 correlation :\n",
    "- `DE_NET_EXPORT` and `DE_NET_IMPORT`\n",
    "- `FR_NET_EXPORT` and `FR_NET_IMPORT`\n",
    "- `DE_FR_EXCHANGE` and `FR_DE_EXCHANGE`\n",
    "\n",
    "Moreover they have the same correlation with the other variables. So keeping both variables doesn't add meaning full information. That is why we chose to drop one of the variables from each -1 correlation.\n",
    "\n",
    "**Remove `FR_COAL` variable**\n",
    "\n",
    "This variable is not diversified. Thus its values are not interesting to keep.\n",
    "\n",
    "**Split the dataset**\n",
    "\n",
    "As decided thanks to the data analysis, we splited the dataset into two : french and german dataset.\n",
    "\n",
    "**Remove Nan Values from both dataset**\n",
    "\n",
    "The proportion of Nan values as well as the few rows we have for each dataset were the reasons why we chose to replace nan values by the median of each column.\n",
    "\n",
    "**Create additionnal columns according to a Threshold**\n",
    "\n",
    "Seuils pour df_fr\n",
    "- COAL_RET < 0.8\n",
    "- FR_CONSUMPTION > 1.5\n",
    "- FR_NUCLEAR < -1.8\n",
    "- FR_HYDRO < -0.4\n",
    "\n",
    "Seuils pour df_de\n",
    "- DE_CONSUMPTION > 1.2\n",
    "- DE_NET_EXPORT > -0.45\n",
    "- DE_WINDPOW > 0.3\n",
    "\n",
    "Transformation \"ReLu\"\n",
    "\n",
    "**Remove Columns that have a low correlation with the TARGET variable**\n",
    "\n",
    "Each variables whose spearman corelation with the `TARGET` variable is lower than 0.05 will be removed from the dataset. We don't consider those variables to have a correlation high enough to have a positive impact on models' performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa2b1a",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfbcb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_fr = {\"COAL_RET\": [0.8, \"inf\"],\n",
    "                \"FR_CONSUMPTION\": [1.5, \"sup\"],\n",
    "                \"FR_NUCLEAR\": [-1.8, \"inf\"],\n",
    "                \"FR_HYDRO\":[-0.4, \"inf\"]                \n",
    "                }\n",
    "\n",
    "threshold_de = {\"DE_CONSUMPTION\": [1.2, \"sup\"],\n",
    "                \"DE_NET_EXPORT\": [-0.45, \"sup\"],\n",
    "                \"DE_WINDPOW\": [0.3, \"sup\"]\n",
    "}\n",
    "\n",
    "# COLONNES RECUPEREES TEMPORAIREMENT A LA MAIN CAR SEPARATIONN DES FICHIERS ANALYSES ET ENGINEERING\n",
    "# A RECUPER DES VARIBALES QUAND LE RASSEMBLEMENT DES FICHIERS SERA FAIT\n",
    "columns_kept_fr = [\"DE_NET_EXPORT\",\n",
    "                \"DE_HYDRO\",\n",
    "                \"DE_WINDPOW\",\n",
    "                \"FR_WINDPOW\",\n",
    "                \"GAS_RET\",\n",
    "                \"CARBON_RET\"]\n",
    "\n",
    "columns_kept_de = [\"DE_NET_EXPORT\",\n",
    "                \"DE_GAS\",\n",
    "                \"DE_COAL\",\n",
    "                \"DE_HYDRO\",\n",
    "                \"DE_WINDPOW\",\n",
    "                \"FR_WINDPOW\",\n",
    "                \"DE_LIGNITE\",\n",
    "                \"DE_RESIDUAL_LOAD\",\n",
    "                \"DE_WIND\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32c2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns):\n",
    "    for c in columns:\n",
    "        df.drop(columns=c, inplace=True, errors=\"ignore\")\n",
    "\n",
    "def compute_median(df):\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    medians = df[numeric_cols].median()\n",
    "    return medians\n",
    "\n",
    "def missing_values_changed_with_median(df, medians):\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(medians[numeric_cols])\n",
    "    return df\n",
    "\n",
    "def add_threshold_columns(df: pd.DataFrame, column_name: str, threshold: float, way: str):\n",
    "    message = column_name + \"_THRESHOLD_\" + str(threshold)\n",
    "    # when way = \"sup\", we want to keep only values that are higher than the threshold\n",
    "    # else we keep the values that are lower than the threshold\n",
    "    if way == \"sup\":\n",
    "        df[message] = df[column_name].where(df[column_name] >= threshold, 0)\n",
    "    else:\n",
    "        df[message] = df[column_name].where(df[column_name] <= threshold, 0)\n",
    "\n",
    "def compute_quantiles(df, low = 0.25, high = 0.75, coeff=5):\n",
    "    bounds = {}\n",
    "    for column in df.select_dtypes(include=[\"number\"]).columns:\n",
    "        Q1 = df[column].quantile(low)\n",
    "        Q3 = df[column].quantile(high)\n",
    "        delta = Q3 - Q1\n",
    "        lower_bound = Q1 - coeff * delta\n",
    "        upper_bound = Q3 + coeff * delta\n",
    "        bounds[column] = (lower_bound, upper_bound)\n",
    "    return bounds\n",
    "\n",
    "def outliers_filter(df, bounds):\n",
    "    filter_ = pd.Series(True, index=df.index)\n",
    "    for column, (low, high) in bounds.items():\n",
    "        if column in df.columns:\n",
    "            filter_ &= (df[column] >= low) & (df[column] <= high)\n",
    "    return filter_\n",
    "\n",
    "def feature_engineering(df, medians, threshold, columns_kept):\n",
    "    # remove unecessary columns\n",
    "    columns_name = [\"DE_NET_IMPORT\", \"FR_NET_IMPORT\", \"DE_FR_EXCHANGE\"]\n",
    "    drop_columns(df, columns_name)\n",
    "\n",
    "    # remove FR_COAL\n",
    "    drop_columns(df, [\"FR_COAL\"])\n",
    "\n",
    "    # modify missing values\n",
    "    df = missing_values_changed_with_median(df, medians)\n",
    "\n",
    "    # add threshold columns to the french dataset\n",
    "    for key, value in threshold.items():\n",
    "        add_threshold_columns(df, key, value[0], value[1])\n",
    "\n",
    "    # drop columns that are not in the list or that have not _THRESHOLD_ in their name\n",
    "    to_keep = [c for c in df.columns if (c in columns_kept) or (\"_THRESHOLD_\" in c)]\n",
    "    df = df[to_keep]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_one_country(df, threshold, columns_kept, standardisation = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"TARGET\"]), df[\"TARGET\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    medians = compute_median(X_train)\n",
    "\n",
    "    X_train = feature_engineering(X_train, medians, threshold, columns_kept)\n",
    "    X_test = feature_engineering(X_test, medians, threshold, columns_kept)\n",
    "\n",
    "    # filter : remove outliers from the train data\n",
    "    bounds = compute_quantiles(X_train)\n",
    "    filter_ = outliers_filter(X_train, bounds)\n",
    "    X_train = X_train[filter_]\n",
    "    y_train = y_train[filter_]\n",
    "\n",
    "    if standardisation:\n",
    "        # Standardisation\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "        # X_train_fr_scaled, X_test_fr_scaled, X_train_de_scaled, X_test_de_scaled are not dataframe, \n",
    "        # we prefer to work with dataframe to keep columns name\n",
    "        X_train = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "        X_test  = pd.DataFrame(X_test_scaled,  index=X_test.index,  columns=X_test.columns)\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def transform(df, threshold_fr, threshold_de, columns_kept_fr, columns_kept_de, standardisation):\n",
    "    # split the dataset\n",
    "    df_fr = df[df[\"COUNTRY\"] == \"FR\"].copy()\n",
    "    df_de = df[df[\"COUNTRY\"] == \"DE\"].copy()\n",
    "\n",
    "    X_train_fr, X_test_fr, y_train_fr, y_test_fr = transform_one_country(\n",
    "        df_fr, threshold_fr, columns_kept_fr, standardisation=standardisation\n",
    "    )\n",
    "\n",
    "    X_train_de, X_test_de, y_train_de, y_test_de = transform_one_country(\n",
    "        df_de, threshold_de, columns_kept_de, standardisation=standardisation\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_fr, X_test_fr, y_train_fr, y_test_fr,\n",
    "        X_train_de, X_test_de, y_train_de, y_test_de\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9d0c6",
   "metadata": {},
   "source": [
    "## Pipeline for all models\n",
    "\n",
    "We observe that if our features engineering seems very relevant for simple and interpretable models, however models that handle better the complexity and non linear relationsip didn't require as feature engineering than a simple linear regression. For that purpose the goal of this part is to do a general pipeline using the last feature engineering pipeline to have a flexible way of testing new models. Furthermore since the observation of an important part of outliers in the French side, make the relationships very noisy, we will remove the extreme outliers, only on training data. We also aim to have the possibilitie to use a different model for France and Allemagne since the optimal model for each could be different. Finally in our objective to avoid overfitting we will use K-fold optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57380a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_corr(y_true, y_pred):\n",
    "    return spearmanr(y_true, y_pred).correlation\n",
    "\n",
    "def kfold_score(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model_ = clone(model)  \n",
    "        model_.fit(X_train, y_train)\n",
    "        y_pred = model_.predict(X_val)\n",
    "\n",
    "        scores.append(spearman_corr(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f159aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_all(\n",
    "    df,\n",
    "    fr_model,\n",
    "    de_model,\n",
    "    threshold_fr=threshold_fr,\n",
    "    threshold_de=threshold_de,\n",
    "    columns_kept_fr=columns_kept_fr,\n",
    "    columns_kept_de=columns_kept_de,\n",
    "    feature_engineering=True,\n",
    "    standardisation=True,\n",
    "    use_grid=False,\n",
    "    k=5,\n",
    "):\n",
    "    # comparison with or without feature engineering\n",
    "    if feature_engineering:\n",
    "        (X_train_fr, X_test_fr, y_train_fr, y_test_fr,\n",
    "            X_train_de, X_test_de, y_train_de, y_test_de) = transform(\n",
    "                                                            df,\n",
    "                                                            threshold_fr=threshold_fr,\n",
    "                                                            threshold_de=threshold_de,\n",
    "                                                            columns_kept_fr=columns_kept_fr,\n",
    "                                                            columns_kept_de=columns_kept_de,\n",
    "                                                            standardisation=standardisation)\n",
    "    else:\n",
    "        df_fr = df[df[\"COUNTRY\"] == \"FR\"].drop(columns=\"COUNTRY\").fillna(0)\n",
    "        df_de = df[df[\"COUNTRY\"] == \"DE\"].drop(columns=\"COUNTRY\").fillna(0)\n",
    "        X_train_fr, X_test_fr, y_train_fr, y_test_fr = train_test_split(df_fr.drop(columns=[\"TARGET\"]), df_fr[\"TARGET\"], test_size=0.2, random_state=42)\n",
    "        X_train_de, X_test_de, y_train_de, y_test_de = train_test_split(df_de.drop(columns=[\"TARGET\"]), df_de[\"TARGET\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    if use_grid:\n",
    "        # france\n",
    "        fr_model.fit(X_train_fr, y_train_fr)\n",
    "        fr_mean = fr_model.best_score_\n",
    "        fr_cv_scores = fr_model.cv_results_[\"mean_test_score\"]\n",
    "        fr_std = fr_cv_scores.std()\n",
    "        fr_estimator = fr_model.best_estimator_\n",
    "\n",
    "        # germany\n",
    "        de_model.fit(X_train_de, y_train_de)\n",
    "        de_mean = de_model.best_score_\n",
    "        de_cv_scores = de_model.cv_results_[\"mean_test_score\"]\n",
    "        de_std = de_cv_scores.std()\n",
    "        de_estimator = de_model.best_estimator_\n",
    "    else:\n",
    "        # k_fold\n",
    "        fr_mean, fr_std = kfold_score(fr_model, X_train_fr, y_train_fr, k=k)\n",
    "        de_mean, de_std = kfold_score(de_model, X_train_de, y_train_de, k=k)\n",
    "\n",
    "        fr_estimator = fr_model\n",
    "        de_estimator = de_model\n",
    "\n",
    "        fr_estimator.fit(X_train_fr, y_train_fr)\n",
    "        de_estimator.fit(X_train_de, y_train_de)\n",
    "\n",
    "    # Test evaluation\n",
    "    y_pred_test_fr = fr_estimator.predict(X_test_fr)\n",
    "    y_pred_test_de = de_estimator.predict(X_test_de)\n",
    "\n",
    "    fr_test_score = spearman_corr(y_test_fr, y_pred_test_fr)\n",
    "    de_test_score = spearman_corr(y_test_de, y_pred_test_de)\n",
    "\n",
    "    # Global Spearman\n",
    "    y_true_global = np.concatenate([y_test_fr, y_test_de])\n",
    "    y_pred_global = np.concatenate([y_pred_test_fr, y_pred_test_de])\n",
    "    spearman_global = spearman_corr(y_true_global, y_pred_global)\n",
    "\n",
    "    return {\n",
    "    \"cv_mode\": \"grid_search\" if use_grid else \"kfold\",\n",
    "    \"fr_cv\": (fr_mean, fr_std),\n",
    "    \"de_cv\": (de_mean, de_std),\n",
    "    \"spearman_fr_test\": fr_test_score,\n",
    "    \"spearman_de_test\": de_test_score,\n",
    "    \"spearman_global_test\": spearman_global,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2102cfc",
   "metadata": {},
   "source": [
    "### Basic Model\n",
    "\n",
    "The first step is to test the simpliest model with almost no feature engineering, to have a sort of reference model and to not considerate all the models less performant. In this first implementation the dataset isn't separate between France and Germany, all the columns are keep and there is no transformation on the columns. The model used is a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddee24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman train : 28.9%\n",
      "Spearman test  : 19.5%\n"
     ]
    }
   ],
   "source": [
    "X_all = df.drop(columns=[\"TARGET\", \"COUNTRY\"]).fillna(0)\n",
    "y_all = df[\"TARGET\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test  = lr.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Spearman train : {:.1f}%\".format(100 * spearman_corr(y_train, y_pred_train)))\n",
    "print(\"Spearman test  : {:.1f}%\".format(100 * spearman_corr(y_test,  y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8828b",
   "metadata": {},
   "source": [
    "## Models with our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44daba9",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cfee0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv_mode': 'kfold',\n",
       " 'fr_cv': (0.20356753106076667, 0.08232517437563475),\n",
       " 'de_cv': (0.24022632190504875, 0.12521555406740847),\n",
       " 'spearman_fr_test': 0.1657655733347872,\n",
       " 'spearman_de_test': 0.392464221824687,\n",
       " 'spearman_global_test': 0.2709933537010029}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_all(df, LinearRegression(), LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c9480",
   "metadata": {},
   "source": [
    "We can see an important improvement of our spearman score, with an amelioration of 8% comparing to the reference model (from 19% to 27%). This justify our global strategy at least for Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f869b",
   "metadata": {},
   "source": [
    "### Polynomiale Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dae8831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv_mode': 'kfold',\n",
       " 'fr_cv': (0.08015539997634188, 0.09902778164029584),\n",
       " 'de_cv': (0.07672971424295011, 0.10003044840653669),\n",
       " 'spearman_fr_test': 0.23626385164754035,\n",
       " 'spearman_de_test': 0.25490831842576034,\n",
       " 'spearman_global_test': 0.2508406763811741}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_model_fr = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "poly_model_de = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline_all(df, poly_model_fr, poly_model_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02458ab0",
   "metadata": {},
   "source": [
    "With polynomial regression, we keep improving our performance, however this model seems adapted only for the french dataset an hybrid model (polynomial regression for the french dataset and linear regression for the deutsh one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a8bf7",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929fbd1",
   "metadata": {},
   "source": [
    "### A simple hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b273dbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv_mode': 'kfold',\n",
       " 'fr_cv': (0.08015539997634188, 0.09902778164029584),\n",
       " 'de_cv': (0.24022632190504875, 0.12521555406740847),\n",
       " 'spearman_fr_test': 0.23626385164754035,\n",
       " 'spearman_de_test': 0.392464221824687,\n",
       " 'spearman_global_test': 0.30924401793090445}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_all(df, poly_model_fr, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42840a9",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "219f2960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv_mode': 'grid_search',\n",
       " 'fr_cv': (0.07108673777153425, 0.03427368833895691),\n",
       " 'de_cv': (0.250134185098134, 0.035309153641252665),\n",
       " 'spearman_fr_test': 0.13872316906362192,\n",
       " 'spearman_de_test': 0.22403112608507902,\n",
       " 'spearman_global_test': 0.18923562853899611}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearman_score = make_scorer(spearman_corr, greater_is_better=True)\n",
    "fr_param_grid = {\n",
    "    \"model__max_depth\": [3, 4, 5, 7],\n",
    "    \"model__min_samples_leaf\": [10, 20, 50],\n",
    "    \"model__min_samples_split\": [10, 20, 30]\n",
    "}\n",
    "\n",
    "fr_base = Pipeline([(\"model\", DecisionTreeRegressor(random_state=42))])\n",
    "\n",
    "fr_search = GridSearchCV(\n",
    "    estimator=fr_base,\n",
    "    param_grid=fr_param_grid,\n",
    "    scoring=spearman_score,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "de_param_grid = {\n",
    "    \"model__max_depth\": [3, 4, 5, 7, 10, 15],\n",
    "    \"model__min_samples_leaf\": [5, 10, 20, 30, 50],\n",
    "    \"model__min_samples_split\": [5, 10, 20]\n",
    "}\n",
    "\n",
    "de_base = Pipeline([(\"model\", DecisionTreeRegressor(random_state=42))])\n",
    "\n",
    "de_search = GridSearchCV(\n",
    "    estimator=de_base,\n",
    "    param_grid=de_param_grid,\n",
    "    scoring=spearman_score,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "pipeline_all(df, fr_model=fr_search, de_model=de_search, use_grid=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603e3bc",
   "metadata": {},
   "source": [
    "We observe poor performance, especially for the German dataset. Essayer d'optimiser ça???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
